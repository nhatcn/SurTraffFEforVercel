import os
import re
import json
import random
import requests
from datetime import datetime
from cachetools import TTLCache
import logging
from bs4 import BeautifulSoup
from fuzzywuzzy import fuzz
import numpy as np
import asyncio
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
import faiss
import google.generativeai as genai
from google.api_core import retry
import psutil
from retry import retry as retry_decorator
import spacy
from typing import List, Dict, Optional
import unicodedata

# Thiáº¿t láº­p logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n file
KNOWLEDGE_TXT_PATH = r"D:\DATN\frontend.16.7\SEP490_SurTraff\backend\services\chatbot\surtraff_knowledge.txt"
SOCIAL_TXT_PATH = r"D:\DATN\frontend.16.7\SEP490_SurTraff\backend\services\chatbot\social.txt"
TRAFFIC_DIALOGS_PATH = r"D:\DATN\frontend.16.7\SEP490_SurTraff\backend\services\chatbot\traffic_dialogs.txt"
FEEDBACK_FILE = r"D:\DATN\frontend.16.7\SEP490_SurTraff\backend\services\chatbot\custom_knowledge.jsonl"
FAISS_INDEX_PATH = r"D:\DATN\frontend.16.7\SEP490_SurTraff\backend\services\chatbot\faiss_index"
CHAT_LOG_FILE = r"D:\DATN\frontend.16.7\SEP490_SurTraff\backend\services\chatbot\chat_log.jsonl"
LIMIT_FEEDBACK = 1000

# Thiáº¿t láº­p API key (mÃ£ hÃ³a trong thá»±c táº¿)
API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyAn_DCoTki5FGn1AJ5E9XyvmbDj9AhoMtw")
genai.configure(api_key=API_KEY)

# Táº£i mÃ´ hÃ¬nh spaCy cho tiáº¿ng Viá»‡t
try:
    nlp_vi = spacy.load("vi_core_news_md")
except:
    logger.warning("KhÃ´ng táº£i Ä‘Æ°á»£c spaCy model vi_core_news_md, bá» qua phÃ¢n tÃ­ch cÃº phÃ¡p")
    nlp_vi = None

# Sá»­ dá»¥ng FAISS-CPU máº·c Ä‘á»‹nh
use_faiss_gpu = False
logger.info("Sá»­ dá»¥ng FAISS-CPU máº·c Ä‘á»‹nh.")

# Danh sÃ¡ch Ä‘á»‹a danh
PLACE_NAMES = [
    "HÃ  Ná»™i", "Há»“ ChÃ­ Minh", "SÃ i GÃ²n", "ÄÃ  Náºµng", "Háº£i PhÃ²ng", "Cáº§n ThÆ¡", "Huáº¿", "Nha Trang",
    "VÅ©ng TÃ u", "ÄÃ  Láº¡t", "BÃ¬nh DÆ°Æ¡ng", "Äá»“ng Nai", "KhÃ¡nh HÃ²a", "Quáº£ng Ninh", "CÃ  Mau",
    "An Giang", "BÃ  Rá»‹a-VÅ©ng TÃ u", "Báº¯c Giang", "Báº¯c Káº¡n", "Báº¡c LiÃªu", "Báº¯c Ninh", "Báº¿n Tre",
    "BÃ¬nh Äá»‹nh", "BÃ¬nh PhÆ°á»›c", "BÃ¬nh Thuáº­n", "Cao Báº±ng", "Äáº¯k Láº¯k", "Äáº¯k NÃ´ng", "Äiá»‡n BiÃªn",
    "Äá»“ng ThÃ¡p", "Gia Lai", "HÃ  Giang", "HÃ  Nam", "HÃ  TÄ©nh", "Háº£i DÆ°Æ¡ng", "Háº­u Giang",
    "HÃ²a BÃ¬nh", "HÆ°ng YÃªn", "KiÃªn Giang", "Kon Tum", "Lai ChÃ¢u", "LÃ¢m Äá»“ng", "Láº¡ng SÆ¡n",
    "LÃ o Cai", "Long An", "Nam Äá»‹nh", "Nghá»‡ An", "Ninh BÃ¬nh", "Ninh Thuáº­n", "PhÃº Thá»",
    "PhÃº YÃªn", "Quáº£ng BÃ¬nh", "Quáº£ng Nam", "Quáº£ng NgÃ£i", "Quáº£ng Trá»‹", "SÃ³c TrÄƒng",
    "SÆ¡n La", "TÃ¢y Ninh", "ThÃ¡i BÃ¬nh", "ThÃ¡i NguyÃªn", "Thanh HÃ³a", "Tiá»n Giang",
    "TrÃ  Vinh", "TuyÃªn Quang", "VÄ©nh Long", "VÄ©nh PhÃºc", "YÃªn BÃ¡i"
]

# Bá»™ nhá»› cache phÃ¢n vÃ¹ng
translation_cache = TTLCache(maxsize=1000, ttl=43200)  # 12 giá»
feedback_cache = TTLCache(maxsize=1000, ttl=86400)     # 1 ngÃ y
semantic_cache = {
    "traffic_law": TTLCache(maxsize=500, ttl=604800),
    "plate_violation": TTLCache(maxsize=500, ttl=3600),
    "traffic_external": TTLCache(maxsize=500, ttl=3600),
    "social": TTLCache(maxsize=200, ttl=86400),
    "general": TTLCache(maxsize=500, ttl=604800)
}
web_cache = TTLCache(maxsize=100, ttl=3600)  # 1 giá»

# Tá»« khÃ³a nghi ngá»
DOUBT_KEYWORDS = ["thiá»‡t", "cháº¯c", "tháº­t", "cÃ³ cháº¯c", "really", "sure", "is it true"]

# Tá»« Ä‘iá»ƒn dá»‹ch thuáº­t
keyword_map = {
    "vÆ°á»£t Ä‘Ã¨n Ä‘á»": "red light violation",
    "giao thÃ´ng": "traffic",
    "mÅ© báº£o hiá»ƒm": "helmet",
    "tai náº¡n": "accident",
    "tá»‘c Ä‘á»™": "speed",
    "Ä‘Ã¨n Ä‘á»": "red light",
    "biá»ƒn sá»‘": "license plate",
    "Ä‘á»— xe sai": "illegal parking",
    "cháº¡y sai lÃ n": "lane violation",
    "ngÆ°á»£c chiá»u": "wrong-way driving",
    "váº­t cáº£n": "obstacle",
    "há»‘ trÃªn Ä‘Æ°á»ng": "pothole",
    "máº­t Ä‘á»™ xe": "traffic density",
    "Ã¹n táº¯c giao thÃ´ng": "traffic jam",
    "quy Ä‘á»‹nh giao thÃ´ng": "traffic regulation",
    "má»©c pháº¡t": "fine",
    "vi pháº¡m giao thÃ´ng": "traffic violation",
    "Ä‘Æ°á»ng cao tá»‘c": "highway",
    "camera giao thÃ´ng": "traffic camera",
    "káº¹t xe": "traffic jam",
    "báº±ng lÃ¡i": "driver's license",
    "Ä‘Ã¨n giao thÃ´ng": "traffic light",
    "Ä‘Æ°á»ng má»™t chiá»u": "one-way road",
    "phÃ¢n luá»“ng giao thÃ´ng": "traffic diversion",
    "cáº£nh sÃ¡t giao thÃ´ng": "traffic police",
    "Ä‘Æ°á»ng quá»‘c lá»™": "national highway",
    "Ä‘Æ°á»ng tá»‰nh lá»™": "provincial road",
    "Ä‘Äƒng kiá»ƒm": "vehicle inspection",
    "xá»­ pháº¡t": "penalize",
    "thá»i gian thá»±c": "real-time",
    "hÃ nh vi": "behavior",
    "bÃ¡o cÃ¡o": "report",
    "báº£n Ä‘á»“": "map",
    "an toÃ n": "safety",
    "giá»›i háº¡n": "limit",
    "tÃ­n hiá»‡u": "signal",
    "Ä‘Æ°á»ng phá»‘": "street",
    "phÃ¢n tÃ­ch": "analysis",
    "yolo": "YOLO",
    "camera": "camera",
    "nháº­n diá»‡n": "recognition",
    "há»‡ thá»‘ng": "system",
    "ngÃ£ tÆ°": "intersection",
    "ngÃ£ ba": "T-junction",
    "cáº§u": "bridge",
    "háº§m": "tunnel",
    "Ä‘oáº¡n Ä‘Æ°á»ng": "road segment",
    "khu vá»±c": "area",
    "vá»‹ trÃ­": "location",
    "hÆ°á»›ng": "direction",
    "bÃ¡o lá»—i há»‡ thá»‘ng": "report system error",
    "Ä‘Äƒng nháº­p": "login",
    "nhÆ° tháº¿ nÃ o": "how",
    "surtraff": "SurTraff"
}

# Tá»« Ä‘iá»ƒn kiá»ƒm tra chÃ­nh táº£
valid_vietnamese_words = set([
    "giao thÃ´ng", "vÆ°á»£t Ä‘Ã¨n Ä‘á»", "mÅ© báº£o hiá»ƒm", "tai náº¡n", "tá»‘c Ä‘á»™", "Ä‘Ã¨n Ä‘á»", "biá»ƒn sá»‘", "Ä‘á»— xe",
    "sai lÃ n", "ngÆ°á»£c chiá»u", "váº­t cáº£n", "há»‘", "máº­t Ä‘á»™", "Ã¹n táº¯c", "pháº¡t", "nghá»‹ Ä‘á»‹nh", "luáº­t",
    "quy Ä‘á»‹nh", "yolo", "camera", "phÃ¡t hiá»‡n", "nháº­n diá»‡n", "káº¹t xe", "báº±ng lÃ¡i", "Ä‘Ã¨n giao thÃ´ng",
    "Ä‘Æ°á»ng má»™t chiá»u", "phÃ¢n luá»“ng", "cáº£nh sÃ¡t", "quá»‘c lá»™", "tá»‰nh lá»™", "Ä‘Äƒng kiá»ƒm", "vi pháº¡m",
    "thá»i gian thá»±c", "phÃ¢n tÃ­ch", "hÃ nh vi", "bÃ¡o cÃ¡o", "báº£n Ä‘á»“", "há»‡ thá»‘ng", "surtraff",
    "Ä‘Æ°á»ng phá»‘", "an toÃ n", "giá»›i háº¡n", "tÃ­n hiá»‡u", "xá»­ pháº¡t", "ngÃ£ tÆ°", "ngÃ£ ba", "cáº§u", "háº§m",
    "Ä‘oáº¡n Ä‘Æ°á»ng", "khu vá»±c", "vá»‹ trÃ­", "hÆ°á»›ng", "bÃ¡o lá»—i", "kiá»ƒm tra", "truy váº¥n", "Ä‘Äƒng nháº­p",
    "nhÆ° tháº¿ nÃ o", "lÃ m sao", "cÃ´ng nghá»‡", "biáº¿t", "tÃ¬nh tráº¡ng", "thÃ´ng", "hÃ´m nay", "tháº¿ nÃ o",
    "phÆ°Æ¡ng tiá»‡n", "xe mÃ¡y", "Ã´ tÃ´", "xe táº£i", "xe khÃ¡ch", "thá»i gian", "ngÃ y", "thÃ¡ng", "nÄƒm",
    "phÃ¡t triá»ƒn", "tÃ¬nh tráº¡ng", "há»i", "tráº£ lá»i", "há»— trá»£", "tiáº¿p tá»¥c", "thÃªm", "liÃªn quan"
])

# Danh sÃ¡ch URL tin tá»©c giao thÃ´ng
TRAFFIC_NEWS_URLS = [
    "https://vnexpress.net/giao-thong",
    "https://thanhnien.vn/giao-thong.htm",
    "https://tuoitre.vn/giao-thong.htm",
    "https://nld.com.vn/giao-thong.htm",
    "https://zingnews.vn/giao-thong.html",
    "https://baocantho.com.vn/"
]

# Kiá»ƒm tra tÃ i nguyÃªn há»‡ thá»‘ng
def check_system_resources():
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        logger.info(f"CPU usage: {cpu_percent}%, Memory usage: {memory_percent}%")
        return cpu_percent <= 85 and memory_percent <= 85
    except Exception as e:
        logger.error(f"Lá»—i kiá»ƒm tra tÃ i nguyÃªn: {e}")
        return False

# Kiá»ƒm tra dung lÆ°á»£ng Ä‘Ä©a
def check_disk_space(path: str, required_mb: int) -> bool:
    try:
        disk = psutil.disk_usage(path)
        free_mb = disk.free / (1024 ** 2)
        logger.info(f"Dung lÆ°á»£ng trá»‘ng táº¡i {path}: {free_mb:.2f} MB")
        return free_mb >= required_mb
    except Exception as e:
        logger.error(f"Lá»—i kiá»ƒm tra dung lÆ°á»£ng Ä‘Ä©a: {e}")
        return False

# Kiá»ƒm tra vÃ  táº¡o file pháº£n há»“i vÃ  log
for file_path in [FEEDBACK_FILE, CHAT_LOG_FILE]:
    if not os.path.exists(file_path):
        with open(file_path, "a", encoding="utf-8") as f:
            pass
        logger.info(f"ÄÃ£ táº¡o file {file_path}")

# Kiá»ƒm tra file Ä‘áº§u vÃ o
for path in [KNOWLEDGE_TXT_PATH, SOCIAL_TXT_PATH, TRAFFIC_DIALOGS_PATH]:
    if not os.path.exists(path):
        logger.warning(f"File khÃ´ng tá»“n táº¡i: {path}")
    else:
        try:
            with open(path, "r", encoding="utf-8-sig") as f:
                content = f.read()
                logger.info(f"File {path}: {len(content)} kÃ½ tá»±")
        except Exception as e:
            logger.error(f"Lá»—i Ä‘á»c file {path}: {e}")

# Kiá»ƒm tra JSONL
def validate_jsonl_file(path: str) -> bool:
    if not os.path.exists(path):
        logger.warning(f"File khÃ´ng tá»“n táº¡i: {path}")
        return False
    try:
        with open(path, "r", encoding="utf-8-sig") as file:
            for i, line in enumerate(file, 1):
                if not line.strip():
                    continue
                try:
                    json.loads(line)
                except json.JSONDecodeError:
                    logger.error(f"DÃ²ng {i} trong {path} khÃ´ng há»£p lá»‡: {line.strip()}")
                    return False
        logger.info(f"File {path} há»£p lá»‡")
        return True
    except Exception as e:
        logger.error(f"Lá»—i kiá»ƒm tra {path}: {e}")
        return False

# HÃ m kiá»ƒm tra Ä‘áº§u vÃ o an toÃ n
def is_safe_input(text: str) -> bool:
    dangerous_patterns = [
        r'<\s*script', r'javascript:', r'sqlmap', r'\bselect\s+.*\s+from\b',
        r'--\s*', r';\s*drop\s+', r';\s*delete\s+', r'\bunion\s+select\b'
    ]
    for pattern in dangerous_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            logger.warning(f"PhÃ¡t hiá»‡n Ä‘áº§u vÃ o nguy hiá»ƒm: {text}")
            return False
    return True

# HÃ m chuáº©n hÃ³a Unicode
def normalize_unicode(text: str) -> str:
    return unicodedata.normalize('NFC', text)

# HÃ m kiá»ƒm tra cÃ¢u há»i tÆ°Æ¡ng tá»± trong cache
def check_similar_question(question: str, cache: TTLCache) -> Optional[str]:
    for key in cache.keys():
        cached_question = key.split(":")[-1]
        if fuzz.ratio(normalize_unicode(question.lower()), normalize_unicode(cached_question.lower())) > 90:
            return cache[key]
    return None

# HÃ m kiá»ƒm tra ngá»¯ cáº£nh liÃªn quan
def check_context_relevance(question: str, history: List[Dict]) -> bool:
    if not history:
        return False
    last_entry = history[-1]
    last_question = last_entry.get("sentence", "").lower()
    question_lower = question.lower()
    if any(keyword in question_lower for keyword in DOUBT_KEYWORDS):
        return True
    if fuzz.ratio(normalize_unicode(question_lower), normalize_unicode(last_question)) > 80:
        return True
    return False

# HÃ m phÃ¢n tÃ­ch cÃº phÃ¡p cÃ¢u há»i
def parse_question(question: str) -> Dict[str, str]:
    result = {"main_verb": "", "entities": [], "vehicle_type": None, "time": None, "intent": "unknown"}
    if not nlp_vi:
        return result
    try:
        doc = nlp_vi(normalize_unicode(question))
        result["main_verb"] = next((token.text for token in doc if token.pos_ == "VERB"), "")
        result["entities"] = [ent.text for ent in doc.ents]
        for token in doc:
            if token.text.lower() in ["xe mÃ¡y", "Ã´ tÃ´", "xe táº£i", "xe khÃ¡ch"]:
                result["vehicle_type"] = token.text
            if token.text.lower() in ["hÃ´m nay", "hÃ´m qua", "ngÃ y mai"] or re.match(r'\d{1,2}/\d{1,2}/\d{4}', token.text):
                result["time"] = token.text
        # PhÃ¢n loáº¡i Ã½ Ä‘á»‹nh
        question_lower = question.lower()
        if any(k in question_lower for k in ["má»©c pháº¡t", "pháº¡t", "nghá»‹ Ä‘á»‹nh"]):
            result["intent"] = "traffic_law"
        elif any(k in question_lower for k in ["biá»ƒn sá»‘", "vi pháº¡m"]):
            result["intent"] = "plate_violation"
        elif any(k in question_lower for k in ["giao thÃ´ng", "káº¹t xe", "máº­t Ä‘á»™"]):
            result["intent"] = "traffic_external"
        elif any(k in question_lower for k in ["chÃ o", "hi", "hello"]):
            result["intent"] = "social"
        return result
    except Exception as e:
        logger.error(f"Lá»—i phÃ¢n tÃ­ch cÃº phÃ¡p: {e}")
        return result

# HÃ m lÃ m sáº¡ch cÃ¢u há»i
def clean_question(sentence: str) -> str:
    if not sentence or not isinstance(sentence, str) or not is_safe_input(sentence):
        return ""
    plate_placeholder = {}
    patterns = [
        r'\b\d{2}[A-Z]?-\d{3,5}\b',
        r'\b\d{2}[A-Z]?-\d{3}\.\d{2}\b',
        r'\b[A-Z]{2}-\d{2}-\d{2,3}\b'
    ]
    sentence = normalize_unicode(sentence)
    for pattern in patterns:
        match = re.search(pattern, sentence, re.IGNORECASE)
        if match:
            plate = match.group(0)
            placeholder = "__PLATE__"
            plate_placeholder[placeholder] = plate
            sentence = sentence.replace(plate, placeholder)
    sentence = re.sub(r'[^\w\sÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µ.,!?]', '', sentence)
    sentence = re.sub(r'\s+', ' ', sentence).strip()
    sentence = auto_correct_spelling(sentence)
    for placeholder, original in plate_placeholder.items():
        sentence = sentence.replace(placeholder, original)
    return sentence

# HÃ m sá»­a lá»—i chÃ­nh táº£ tá»± Ä‘á»™ng
def auto_correct_spelling(text: str) -> str:
    if not text:
        return text
    text = normalize_unicode(text)
    text = re.sub(r'(\s*ğŸ˜Š\s*)+', ' ğŸ˜Š', text)
    text = re.sub(r'(\w+)\s+\1', r'\1', text, flags=re.IGNORECASE)
    text = re.sub(r'[^\w\sÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µ.,!?]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    corrections = [
        ("Ä‘Æ°á»ng má»™t chiá»ƒu", "Ä‘Æ°á»ng má»™t chiá»u"), ("giao thong", "giao thÃ´ng"), ("giao thongg", "giao thÃ´ng"),
        ("káº¹t xáº¹", "káº¹t xe"), ("má»§ báº£o hiá»ƒm", "mÅ© báº£o hiá»ƒm"), ("tai náº¡m", "tai náº¡n"), ("tá»‘c Ä‘Ã´", "tá»‘c Ä‘á»™"),
        ("Ä‘Ã¨n Ä‘o", "Ä‘Ã¨n Ä‘á»"), ("surtraff error", "bÃ¡o lá»—i há»‡ thá»‘ng"), ("phÃ¡triá»ƒn", "phÃ¡t triá»ƒn"),
        ("biáº¿thÃªm", "biáº¿t thÃªm"), ("cÃ´nghá»‡", "cÃ´ng nghá»‡"), ("tinÃ y", "tin nÃ y"), ("Ráº¥tiáº¿c", "Ráº¥t tiáº¿c"),
        ("tÃ¬nh tráº¡ngiao", "tÃ¬nh tráº¡ng"), ("há»iÄ‘Ã¡p", "há»i Ä‘Ã¡p"), ("tráº£lá»i", "tráº£ lá»i"), ("há»—trá»£", "há»— trá»£"),
        ("tiáº¿ptá»¥c", "tiáº¿p tá»¥c"), ("thÃªmthÃ´ngtin", "thÃªm thÃ´ng tin"), ("liÃªnquan", "liÃªn quan")
    ]
    for wrong, correct in corrections:
        text = re.sub(r'\b' + re.escape(wrong) + r'\b', correct, text, flags=re.IGNORECASE)
    if nlp_vi:
        doc = nlp_vi(text)
        corrected = " ".join(token.text for token in doc if token.text.lower() in valid_vietnamese_words or not token.is_alpha)
        return corrected if corrected.strip() else text
    return text

# HÃ m kiá»ƒm tra chÃ­nh táº£ tiáº¿ng Viá»‡t
def check_vietnamese_spelling(text: str) -> bool:
    if not text or not isinstance(text, str):
        return False
    text = normalize_unicode(text)
    text = re.sub(r'[^\w\sÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µ]', '', text)
    words = text.lower().split()
    if not words:
        return False
    return any(word in valid_vietnamese_words for word in words)

# HÃ m ghi log trÃ² chuyá»‡n
def log_chat(question: str, response: str, lang: str, topic: str, plate: str = None):
    try:
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "question": normalize_unicode(question),
            "response": normalize_unicode(response),
            "language": lang,
            "topic": topic,
            "license_plate": plate,
            "stack_trace": ""
        }
        with open(CHAT_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        logger.info(f"ÄÃ£ ghi log cho cÃ¢u há»i: {question[:50]}... (plate: {plate})")
    except Exception as e:
        logger.error(f"Lá»—i ghi log: {e}")

# HÃ m lÆ°u pháº£n há»“i
def save_feedback(question: str, response: str, lang: str):
    try:
        feedback_entry = {
            "content": normalize_unicode(response),
            "question": normalize_unicode(question),
            "language": lang,
            "timestamp": datetime.now().isoformat(),
            "topic": detect_topic(question)
        }
        with open(FEEDBACK_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(feedback_entry, ensure_ascii=False) + "\n")
        logger.info(f"ÄÃ£ lÆ°u pháº£n há»“i cho cÃ¢u há»i: {question[:50]}...")
        asyncio.create_task(update_user_index())
    except Exception as e:
        logger.error(f"Lá»—i lÆ°u pháº£n há»“i: {e}")

# HÃ m táº¡o nhÃºng vá»›i Gemini API
@retry_decorator(tries=4, delay=1, backoff=2)
def get_gemini_embeddings(texts: List[str], model: str = "text-embedding-004", task_type: str = "SEMANTIC_SIMILARITY", output_dimensionality: int = 512) -> List[np.ndarray]:
    try:
        if isinstance(texts, str):
            texts = [texts]
        texts = [normalize_unicode(text[:1500]) for text in texts if text.strip()]
        if not texts:
            logger.error("Danh sÃ¡ch vÄƒn báº£n rá»—ng hoáº·c khÃ´ng há»£p lá»‡")
            return []
        embeddings = genai.embed_content(
            model=model,
            content=texts,
            task_type=task_type,
            output_dimensionality=output_dimensionality
        )['embedding']
        embeddings = [np.array(e, dtype='float32') for e in embeddings if len(e) == output_dimensionality]
        if not embeddings:
            logger.error("Gemini API tráº£ vá» embeddings rá»—ng")
            return []
        embeddings = [e / np.linalg.norm(e) if np.linalg.norm(e) != 0 else e for e in embeddings]
        return embeddings
    except Exception as e:
        logger.error(f"Lá»—i khi táº¡o nhÃºng vá»›i Gemini API: {e}")
        raise

# HÃ m trÃ­ch xuáº¥t vÄƒn báº£n tá»« file
def extract_text_from_txt(file_path: str, prioritize_dialogs: bool = False) -> str:
    try:
        if prioritize_dialogs and file_path == KNOWLEDGE_TXT_PATH and os.path.exists(TRAFFIC_DIALOGS_PATH):
            if validate_jsonl_file(TRAFFIC_DIALOGS_PATH):
                dialog_text = []
                with open(TRAFFIC_DIALOGS_PATH, "r", encoding="utf-8-sig") as f:
                    for line in f:
                        if not line.strip():
                            continue
                        try:
                            entry = json.loads(line)
                            question = entry.get("question", "")
                            answers = entry.get("answers", [])
                            if question and answers:
                                dialog_text.append(f"{question} {' '.join(answers)}")
                        except json.JSONDecodeError:
                            logger.warning(f"JSON khÃ´ng há»£p lá»‡: {line.strip()}")
                dialog_text = " ".join(set(dialog_text))
                if dialog_text.strip():
                    return normalize_unicode(dialog_text)
        
        with open(file_path, "r", encoding="utf-8-sig") as f:
            text = f.read()
        text = normalize_unicode(text)
        text = auto_correct_spelling(text)
        return text
    except Exception as e:
        logger.error(f"Lá»—i trÃ­ch xuáº¥t vÄƒn báº£n tá»« {file_path}: {e}")
        return ""

# HÃ m trÃ­ch xuáº¥t vÄƒn báº£n tá»« URL
@retry_decorator(tries=4, delay=1, backoff=2)
def extract_text_from_url(url: str, max_chars: int = 1500) -> str:
    cache_key = f"web:{url}"
    if cache_key in web_cache:
        logger.info(f"Cache hit cho URL: {url}")
        return web_cache[cache_key]
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        
        for element in soup(["script", "style", "header", "footer", "nav", "aside"]):
            element.decompose()
        
        text_elements = soup.find_all(["p", "h1", "h2", "h3"])
        text = " ".join([elem.get_text(strip=True) for elem in text_elements])
        text = normalize_unicode(text)
        text = re.sub(r'\s+', ' ', text).strip()
        text = text[:max_chars]
        text = auto_correct_spelling(text)
        
        web_cache[cache_key] = text
        logger.info(f"ÄÃ£ trÃ­ch xuáº¥t {len(text)} kÃ½ tá»± tá»« {url}")
        return text
    except requests.exceptions.RequestException as e:
        logger.error(f"Lá»—i trÃ­ch xuáº¥t tá»« {url}: {e}")
        return ""

# HÃ m phÃ¡t hiá»‡n chá»§ Ä‘á»
def detect_topic(text: str) -> str:
    if not text.strip():
        return "General"
    text = normalize_unicode(text.lower())
    if any(k in text for k in ["pháº¡t", "má»©c pháº¡t", "nghá»‹ Ä‘á»‹nh", "luáº­t", "báº±ng lÃ¡i"]): return "Traffic Law"
    if any(k in text for k in ["cÃ¡c chá»©c nÄƒng", "chá»©c nÄƒng phÃ¡t hiá»‡n", "detection functions"]): return "SurTraff Functions"
    if any(k in text for k in ["Ä‘Ã¨n Ä‘á»", "red light"]): return "Red Light Detection"
    if any(k in text for k in ["tá»‘c Ä‘á»™", "speed", "overspeed"]): return "Speed Violation"
    if any(k in text for k in ["mÅ© báº£o hiá»ƒm", "helmet", "no helmet"]): return "Helmet Violation"
    if any(k in text for k in ["tai náº¡n", "accident"]): return "Accident Detection"
    if any(k in text for k in ["há»‘", "váº­t cáº£n", "obstacle", "pothole"]): return "Obstacle Detection"
    if any(k in text for k in ["Ä‘á»— xe", "parking"]): return "Illegal Parking"
    if any(k in text for k in ["sai lÃ n", "ngÆ°á»£c chiá»u", "lane", "wrong-way"]): return "Lane Violation"
    if any(k in text for k in ["máº­t Ä‘á»™", "Ã¹n táº¯c", "traffic density"]): return "Traffic Density"
    if any(k in text for k in ["surtraff", "há»‡ thá»‘ng", "Ä‘Äƒng nháº­p", "login"]): return "SurTraff System"
    if any(k in text for k in PLACE_NAMES): return "Traffic Information"
    if any(k in text for k in ["yolo", "camera", "phÃ¡t hiá»‡n", "nháº­n diá»‡n", "biá»ƒn sá»‘", "vi pháº¡m", "giao thÃ´ng thá»i gian thá»±c"]): return "SurTraff Feature"
    return "General"

# Danh sÃ¡ch chi tiáº¿t chá»©c nÄƒng SurTraff
def build_surtraff_details():
    details = {
        "Red Light Detection": "SurTraff sá»­ dá»¥ng camera AI Ä‘á»“ng bá»™ vá»›i tÃ­n hiá»‡u Ä‘Ã¨n giao thÃ´ng Ä‘á»ƒ phÃ¡t hiá»‡n xe vÆ°á»£t Ä‘Ã¨n Ä‘á», Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c trÃªn 90%.",
        "Speed Violation": "SurTraff Ä‘o tá»‘c Ä‘á»™ xe báº±ng cÃ¡ch theo dÃµi khoáº£ng cÃ¡ch giá»¯a hai Ä‘iá»ƒm, so sÃ¡nh vá»›i giá»›i háº¡n tá»‘c Ä‘á»™ cá»§a Ä‘oáº¡n Ä‘Æ°á»ng.",
        "Helmet Violation": "SurTraff sá»­ dá»¥ng AI YOLOv8 Ä‘á»ƒ phÃ¡t hiá»‡n tÃ i xáº¿ hoáº·c hÃ nh khÃ¡ch khÃ´ng Ä‘á»™i mÅ© báº£o hiá»ƒm, gá»­i cáº£nh bÃ¡o thá»i gian thá»±c.",
        "Accident Detection": "SurTraff phÃ¢n tÃ­ch chuyá»ƒn Ä‘á»™ng vÃ  Ä‘á»‘i tÆ°á»£ng Ä‘á»ƒ phÃ¡t hiá»‡n va cháº¡m, kÃ­ch hoáº¡t cáº£nh bÃ¡o kháº©n cáº¥p vÃ  há»— trá»£ phÃ¢n tÃ­ch video tai náº¡n.",
        "Illegal Parking": "SurTraff phÃ¡t hiá»‡n xe Ä‘á»— á»Ÿ khu vá»±c cáº¥m quÃ¡ 3 phÃºt, tá»± Ä‘á»™ng ghi nháº­n vi pháº¡m.",
        "Lane Violation": "SurTraff phÃ¡t hiá»‡n xe Ä‘i sai lÃ n hoáº·c ngÆ°á»£c chiá»u báº±ng cÃ¡ch theo dÃµi Ä‘á»‘i tÆ°á»£ng, Ä‘ang thá»­ nghiá»‡m á»Ÿ má»™t sá»‘ khu vá»±c.",
        "Obstacle Detection": "SurTraff phÃ¡t hiá»‡n há»‘, cÃ¢y Ä‘á»• hoáº·c váº­t cáº£n trÃªn Ä‘Æ°á»ng, cáº­p nháº­t trÃªn báº£n Ä‘á»“ há»‡ thá»‘ng.",
        "Traffic Density": "SurTraff phÃ¢n tÃ­ch máº­t Ä‘á»™ xe qua camera, cung cáº¥p cáº£nh bÃ¡o Ã¹n táº¯c vÃ  dá»± Ä‘oÃ¡n táº¯c Ä‘Æ°á»ng thá»i gian thá»±c.",
        "SurTraff Functions": "SurTraff há»— trá»£ phÃ¡t hiá»‡n vÆ°á»£t Ä‘Ã¨n Ä‘á», vÆ°á»£t tá»‘c Ä‘á»™, khÃ´ng Ä‘á»™i mÅ© báº£o hiá»ƒm, Ä‘á»— xe sai, cháº¡y sai lÃ n/ngÆ°á»£c chiá»u, tai náº¡n, váº­t cáº£n, máº­t Ä‘á»™ xe, vÃ  nháº­n diá»‡n biá»ƒn sá»‘.",
        "Traffic Law": """Theo Nghá»‹ Ä‘á»‹nh 168/2024/NÄ-CP (hiá»‡u lá»±c 1/1/2025):
        - VÆ°á»£t Ä‘Ã¨n Ä‘á»: Xe mÃ¡y 800.000-1.200.000 Ä‘á»“ng, Ã´ tÃ´ 4-6 triá»‡u Ä‘á»“ng.
        - KhÃ´ng Ä‘á»™i mÅ© báº£o hiá»ƒm: 400.000-600.000 Ä‘á»“ng.
        - Cháº¡y quÃ¡ tá»‘c Ä‘á»™:
          + Xe mÃ¡y: VÆ°á»£t 5-<10 km/h: 400.000-600.000 Ä‘á»“ng; 10-20 km/h: 800.000-1.000.000 Ä‘á»“ng; >20 km/h: 6-8 triá»‡u Ä‘á»“ng.
          + Ã” tÃ´: VÆ°á»£t 5-<10 km/h: 800.000-1.000.000 Ä‘á»“ng; 10-20 km/h: 4-6 triá»‡u Ä‘á»“ng; 20-35 km/h: 6-8 triá»‡u Ä‘á»“ng; >35 km/h: 12-14 triá»‡u Ä‘á»“ng.
        - Äi ngÆ°á»£c chiá»u: Xe mÃ¡y 400.000-600.000 Ä‘á»“ng, Ã´ tÃ´ 2-4 triá»‡u Ä‘á»“ng.
        - Äá»— xe sai: Xe mÃ¡y 400.000-600.000 Ä‘á»“ng, Ã´ tÃ´ 800.000-1.200.000 Ä‘á»“ng.""",
        "SurTraff Feature": "SurTraff sá»­ dá»¥ng YOLOv8 vÃ  camera AI Ä‘á»ƒ nháº­n diá»‡n biá»ƒn sá»‘ xe, phÃ¢n tÃ­ch hÃ nh vi lÃ¡i xe nguy hiá»ƒm, quáº£n lÃ½ Ä‘Ã¨n giao thÃ´ng, cung cáº¥p báº£n Ä‘á»“ giao thÃ´ng thá»i gian thá»±c, vÃ  bÃ¡o cÃ¡o vi pháº¡m theo khu vá»±c.",
        "SurTraff System": "SurTraff cho phÃ©p Ä‘Äƒng nháº­p qua tÃ i khoáº£n Ä‘Æ°á»£c cáº¥p bá»Ÿi há»‡ thá»‘ng, sá»­ dá»¥ng á»©ng dá»¥ng hoáº·c website chÃ­nh thá»©c vá»›i email vÃ  máº­t kháº©u."
    }
    if validate_jsonl_file(TRAFFIC_DIALOGS_PATH):
        with open(TRAFFIC_DIALOGS_PATH, "r", encoding="utf-8-sig") as file:
            for line in file:
                if not line.strip():
                    continue
                try:
                    entry = json.loads(line)
                    question = entry.get("question", "").lower()
                    answers = entry.get("answers", [])
                    topic = detect_topic(question)
                    if topic not in details and answers:
                        details[topic] = " ".join(answers)[:150]
                except json.JSONDecodeError:
                    logger.warning(f"JSON khÃ´ng há»£p lá»‡: {line.strip()}")
                    continue
    return details

surtraff_details = build_surtraff_details()

# Text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)

# HÃ m phÃ¡t hiá»‡n ngÃ´n ngá»¯
async def detect_language(text: str, history: List[Dict]) -> str:
    if not text.strip():
        return "vi"
    text_lower = normalize_unicode(text.lower())
    vi_keywords = ["giao thÃ´ng", "biá»ƒn sá»‘", "mÅ© báº£o hiá»ƒm", "Ä‘Ã¨n Ä‘á»", "tá»‘c Ä‘á»™", "Ä‘Äƒng nháº­p", "nhÆ° tháº¿ nÃ o"]
    en_keywords = ["traffic", "license plate", "helmet", "red light", "speed", "login", "how"]
    
    vi_count = sum(1 for k in vi_keywords if k in text_lower)
    en_count = sum(1 for k in en_keywords if k in text_lower)
    
    if vi_count > 0 and en_count == 0:
        return "vi"
    if en_count > 0 and vi_count == 0:
        return "en"
    if vi_count > 0 or en_count > 0:
        return "mixed"
    if history:
        return history[-1].get("lang", "vi")
    
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            f"Detect the language of this text: '{text}'. Return only 'vi' for Vietnamese or 'en' for English.",
            generation_config={"max_output_tokens": 10, "temperature": 0.0}
        )
        lang = response.text.strip()
        return lang if lang in ["vi", "en"] else "vi"
    except Exception as e:
        logger.error(f"Lá»—i phÃ¡t hiá»‡n ngÃ´n ngá»¯: {e}")
        return "vi"

# HÃ m dá»‹ch thuáº­t vi->en
async def translate_vi2en(text: str) -> str:
    cache_key = f"vi2en:{normalize_unicode(text)}"
    if cache_key in translation_cache:
        return translation_cache[cache_key]
    try:
        place_map = {}
        text_lower = normalize_unicode(text.lower())
        for vi, en in keyword_map.items():
            if vi in text_lower:
                placeholder = f"__KEYWORD_{len(place_map)}__"
                place_map[placeholder] = en
                text = re.sub(r'\b' + re.escape(vi) + r'\b', placeholder, text, flags=re.IGNORECASE)
                text_lower = text_lower.replace(vi.lower(), placeholder.lower())
        for place in PLACE_NAMES:
            if place.lower() in text_lower:
                placeholder = f"__PLACE_{len(place_map)}__"
                place_map[placeholder] = place
                text = re.sub(r'\b' + re.escape(place) + r'\b', placeholder, text, flags=re.IGNORECASE)
                text_lower = text_lower.replace(place.lower(), placeholder.lower())
        
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            f"Translate the following Vietnamese text to English, preserving traffic-related terms: {text}",
            generation_config={"max_output_tokens": 150, "temperature": 0.7}
        )
        translated = normalize_unicode(response.text.strip())
        translated = auto_correct_spelling(translated)
        for placeholder, original in place_map.items():
            translated = translated.replace(placeholder, original)
        translated = re.sub(r'\s+', ' ', translated).strip()
        translation_cache[cache_key] = translated
        return translated
    except Exception as e:
        logger.error(f"Lá»—i dá»‹ch vi->en: {e}")
        return text if check_vietnamese_spelling(text) else "KhÃ´ng thá»ƒ dá»‹ch cÃ¢u nÃ y, vui lÃ²ng thá»­ láº¡i!"

# HÃ m dá»‹ch thuáº­t en->vi
async def translate_en2vi(text: str) -> str:
    cache_key = f"en2vi:{normalize_unicode(text)}"
    if cache_key in translation_cache:
        return translation_cache[cache_key]
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            f"Translate the following English text to Vietnamese, preserving traffic-related terms: {text}",
            generation_config={"max_output_tokens": 150, "temperature": 0.7}
        )
        translated = normalize_unicode(response.text.strip())
        translated = auto_correct_spelling(translated)
        translated = re.sub(r'\s+', ' ', translated).strip()
        if not check_vietnamese_spelling(translated):
            logger.warning(f"Dá»‹ch en->vi khÃ´ng há»£p lá»‡: {translated}")
            return "KhÃ´ng thá»ƒ dá»‹ch cÃ¢u nÃ y, vui lÃ²ng thá»­ láº¡i!"
        translation_cache[cache_key] = translated
        return translated
    except Exception as e:
        logger.error(f"Lá»—i dá»‹ch en->vi: {e}")
        return "KhÃ´ng thá»ƒ dá»‹ch cÃ¢u nÃ y, vui lÃ²ng thá»­ láº¡i!"

# HÃ m láº¥y thá»i gian trong ngÃ y
def get_time_of_day() -> str:
    current_hour = datetime.now().hour
    if 5 <= current_hour < 12:
        return "morning"
    elif 12 <= current_hour < 17:
        return "afternoon"
    elif 17 <= current_hour < 21:
        return "evening"
    else:
        return "night"

# HÃ m phÃ¡t hiá»‡n cáº£m xÃºc
def detect_emotion(text: str) -> str:
    text_lower = normalize_unicode(text.lower())
    if any(word in text_lower for word in ["gáº¥p", "kháº©n cáº¥p", "nguy hiá»ƒm", "urgent", "emergency"]):
        return "urgent"
    if any(word in text_lower for word in ["vui", "happy", "tá»‘t", "good", "ğŸ˜Š", "ğŸ˜„"]):
        return "positive"
    if any(word in text_lower for word in ["tá»‡", "xáº¥u", "bad", "terrible", "ğŸ˜”", "ğŸ˜¢"]):
        return "negative"
    return "neutral"

# HÃ m tÃ³m táº¯t ngá»¯ cáº£nh
async def summarize_context(history: List[Dict]) -> str:
    if not history:
        return ""
    history_text = " ".join([entry.get("sentence", "") for entry in history])[:1000]
    if not history_text.strip() or len(history_text.split()) < 5:
        return ""
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            f"TÃ³m táº¯t ngáº¯n gá»n ngá»¯ cáº£nh tá»« cÃ¡c cÃ¢u há»i sau báº±ng tiáº¿ng Viá»‡t (tá»‘i Ä‘a 30 tá»«): {normalize_unicode(history_text)}",
            generation_config={"max_output_tokens": 30, "temperature": 0.7}
        )
        return normalize_unicode(response.text.strip())
    except Exception as e:
        logger.error(f"Lá»—i tÃ³m táº¯t ngá»¯ cáº£nh: {e}")
        return ""

# HÃ m táº¡o cÃ¢u há»i gá»£i Ã½
async def generate_suggested_questions(history: List[Dict], topic: str, lang: str) -> str:
    try:
        suggestions = {
            "Traffic Law": [
                "Má»©c pháº¡t vÆ°á»£t Ä‘Ã¨n Ä‘á» cho xe mÃ¡y lÃ  bao nhiÃªu?",
                "Cháº¡y quÃ¡ tá»‘c Ä‘á»™ á»Ÿ thÃ nh phá»‘ bá»‹ pháº¡t tháº¿ nÃ o?",
                "Luáº­t giao thÃ´ng má»›i nháº¥t lÃ  gÃ¬?",
                "Pháº¡t khÃ´ng Ä‘á»™i mÅ© báº£o hiá»ƒm bao nhiÃªu tiá»n?"
            ],
            "SurTraff Feature": [
                "SurTraff phÃ¡t hiá»‡n vi pháº¡m giao thÃ´ng nhÆ° tháº¿ nÃ o?",
                "Camera AI cá»§a SurTraff hoáº¡t Ä‘á»™ng ra sao?",
                "SurTraff cÃ³ há»— trá»£ báº£n Ä‘á»“ giao thÃ´ng thá»i gian thá»±c khÃ´ng?",
                "SurTraff nháº­n diá»‡n biá»ƒn sá»‘ xe tháº¿ nÃ o?"
            ],
            "Traffic Information": [
                "TÃ¬nh tráº¡ng giao thÃ´ng á»Ÿ HÃ  Ná»™i hÃ´m nay ra sao?",
                "CÃ³ káº¹t xe á»Ÿ Cáº§n ThÆ¡ khÃ´ng?",
                "ÄÆ°á»ng nÃ o á»Ÿ ÄÃ  Náºµng Ä‘ang sá»­a chá»¯a?",
                "Máº­t Ä‘á»™ giao thÃ´ng á»Ÿ Há»“ ChÃ­ Minh tháº¿ nÃ o?"
            ],
            "General": [
                "SurTraff lÃ  gÃ¬?",
                "LÃ m sao Ä‘á»ƒ Ä‘Äƒng nháº­p vÃ o SurTraff?",
                "SurTraff há»— trá»£ nhá»¯ng tÃ­nh nÄƒng gÃ¬?",
                "Há»‡ thá»‘ng SurTraff hoáº¡t Ä‘á»™ng á»Ÿ Ä‘Ã¢u?"
            ]
        }
        topic_suggestions = suggestions.get(topic, suggestions["General"])
        if history:
            current_question = history[-1].get("sentence", "").lower()
            topic_suggestions = [s for s in topic_suggestions if fuzz.ratio(normalize_unicode(s.lower()), normalize_unicode(current_question)) < 90]
        suggestion = random.choice(topic_suggestions) if topic_suggestions else "Há»i thÃªm vá» giao thÃ´ng nhÃ©!"
        if history:
            suggestion = f"Tiáº¿p ná»‘i cÃ¢u há»i trÆ°á»›c, báº¡n cÃ³ muá»‘n biáº¿t: {suggestion}"
        else:
            suggestion = f"Gá»£i Ã½: {suggestion}"
        if lang == "en":
            suggestion = await translate_vi2en(suggestion)
        return normalize_unicode(suggestion)
    except Exception as e:
        logger.error(f"Lá»—i táº¡o cÃ¢u há»i gá»£i Ã½: {e}")
        return "Gá»£i Ã½: Há»i thÃªm vá» giao thÃ´ng hoáº·c SurTraff nhÃ©!" if lang == "vi" else "Suggestion: Ask more about traffic or SurTraff!"

# HÃ m tráº£ lá»i xÃ£ há»™i
async def get_social_response(question: str, lang: str, time_of_day: str, history: List[Dict], emotion: str) -> str:
    try:
        greetings = {
            "morning": {
                "positive": "ChÃ o buá»•i sÃ¡ng! HÃ´m nay báº¡n vui váº», muá»‘n biáº¿t thÃªm vá» giao thÃ´ng khÃ´ng? ğŸ˜Š" if lang == "vi" else "Good morning! You're cheerful today, want to know more about traffic? ğŸ˜Š",
                "neutral": "ChÃ o buá»•i sÃ¡ng! HÃ´m nay báº¡n muá»‘n há»i gÃ¬ vá» giao thÃ´ng? ğŸ˜Š" if lang == "vi" else "Good morning! What do you want to ask about traffic today? ğŸ˜Š",
                "negative": "ChÃ o buá»•i sÃ¡ng! CÃ³ gÃ¬ khÃ´ng á»•n Ã ? Há»i vá» giao thÃ´ng Ä‘á»ƒ mÃ¬nh giÃºp nhÃ©! ğŸ˜Š" if lang == "vi" else "Good morning! Something wrong? Ask about traffic, I'll help! ğŸ˜Š",
                "urgent": "ChÃ o buá»•i sÃ¡ng! Cáº§n thÃ´ng tin giao thÃ´ng gáº¥p Ã ? Há»i ngay nÃ o! ğŸš¨" if lang == "vi" else "Good morning! Need traffic info urgently? Ask now! ğŸš¨"
            },
            "afternoon": {
                "positive": "ChÃ o buá»•i chiá»u! TÃ¢m tráº¡ng tá»‘t nhá»‰, há»i gÃ¬ vá» giao thÃ´ng nÃ o? ğŸš—" if lang == "vi" else "Good afternoon! Feeling great, what's your traffic question? ğŸš—",
                "neutral": "ChÃ o buá»•i chiá»u! CÃ³ cáº§n thÃ´ng tin giao thÃ´ng khÃ´ng? ğŸš—" if lang == "vi" else "Good afternoon! Need traffic information? ğŸš—",
                "negative": "ChÃ o buá»•i chiá»u! CÃ³ gÃ¬ khÃ´ng á»•n? Há»i vá» giao thÃ´ng Ä‘á»ƒ mÃ¬nh há»— trá»£ nhÃ©! ğŸ˜Š" if lang == "vi" else "Good afternoon! Something wrong? Ask about traffic for help! ğŸ˜Š",
                "urgent": "ChÃ o buá»•i chiá»u! Cáº§n thÃ´ng tin giao thÃ´ng gáº¥p Ã ? Há»i ngay nÃ o! ğŸš¨" if lang == "vi" else "Good afternoon! Urgent traffic info needed? Ask now! ğŸš¨"
            },
            "evening": {
                "positive": "ChÃ o buá»•i tá»‘i! Vui váº» tháº¿, há»i gÃ¬ vá» SurTraff nÃ o? ğŸŒ™" if lang == "vi" else "Good evening! So cheerful, what's up with SurTraff? ğŸŒ™",
                "neutral": "ChÃ o buá»•i tá»‘i! Há»i gÃ¬ vá» SurTraff nÃ o? ğŸŒ™" if lang == "vi" else "Good evening! What's up with SurTraff? ğŸŒ™",
                "negative": "ChÃ o buá»•i tá»‘i! CÃ³ gÃ¬ khÃ´ng á»•n Ã ? Há»i vá» giao thÃ´ng Ä‘á»ƒ mÃ¬nh giÃºp nhÃ©! ğŸ˜Š" if lang == "vi" else "Good evening! Something wrong? Ask about traffic for help! ğŸ˜Š",
                "urgent": "ChÃ o buá»•i tá»‘i! Cáº§n thÃ´ng tin giao thÃ´ng gáº¥p Ã ? Há»i ngay nÃ o! ğŸš¨" if lang == "vi" else "Good evening! Urgent traffic info needed? Ask now! ğŸš¨"
            },
            "night": {
                "positive": "Khuya rá»“i, váº«n vui váº» Ã ? Há»i gÃ¬ vá» giao thÃ´ng nÃ o! ğŸŒŒ" if lang == "vi" else "It's late, still cheerful? Ask about traffic! ğŸŒŒ",
                "neutral": "Khuya rá»“i, váº«n quan tÃ¢m giao thÃ´ng Ã ? Há»i Ä‘i! ğŸŒŒ" if lang == "vi" else "It's late! Still curious about traffic? Ask away! ğŸŒŒ",
                "negative": "Khuya rá»“i, cÃ³ gÃ¬ khÃ´ng á»•n Ã ? Há»i vá» giao thÃ´ng Ä‘á»ƒ mÃ¬nh há»— trá»£ nhÃ©! ğŸ˜Š" if lang == "vi" else "It's late! Something wrong? Ask about traffic for help! ğŸ˜Š",
                "urgent": "Khuya rá»“i, cáº§n thÃ´ng tin giao thÃ´ng gáº¥p Ã ? Há»i ngay nÃ o! ğŸš¨" if lang == "vi" else "It's late! Urgent traffic info needed? Ask now! ğŸš¨"
            }
        }
        greeting = greetings.get(time_of_day, {}).get(emotion, "ChÃ o báº¡n! Há»i gÃ¬ vá» giao thÃ´ng nhÃ©! ğŸ˜Š" if lang == "vi" else "Hello! Ask about traffic! ğŸ˜Š")
        if history:
            greeting = f"{greeting} Tiáº¿p ná»‘i cÃ¢u há»i trÆ°á»›c, báº¡n muá»‘n biáº¿t thÃªm gÃ¬? ğŸ˜Š" if lang == "vi" else f"{greeting} Following your last question, what else do you want to know? ğŸ˜Š"
        return normalize_unicode(greeting)
    except Exception as e:
        logger.error(f"Lá»—i tráº£ lá»i xÃ£ há»™i: {e}")
        return "ChÃ o báº¡n! Há»i gÃ¬ vá» giao thÃ´ng nhÃ©! ğŸ˜Š" if lang == "vi" else "Hello! Ask about traffic! ğŸ˜Š"

# HÃ m trÃ­ch xuáº¥t biá»ƒn sá»‘
def extract_plate(text: str) -> Optional[str]:
    patterns = [
        r'\b\d{2}[A-Z]?-\d{3,5}\b',
        r'\b\d{2}[A-Z]?-\d{3}\.\d{2}\b',
        r'\b[A-Z]{2}-\d{2}-\d{2,3}\b'
    ]
    text = normalize_unicode(text)
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            plate = match.group(0).upper()
            if re.match(r'^\d{2}[A-Z]?-\d{3,5}$', plate) or re.match(r'^\d{2}[A-Z]?-\d{3}\.\d{2}$', plate) or re.match(r'^[A-Z]{2}-\d{2}-\d{2,3}$', plate):
                return plate
    return None

# HÃ m kiá»ƒm tra cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i
async def check_answer_quality(question: str, answer: str, lang: str) -> float:
    if len(answer.split()) < 5:
        logger.warning(f"CÃ¢u tráº£ lá»i quÃ¡ ngáº¯n: {answer}")
        return 0.0
    try:
        embeddings = get_gemini_embeddings([normalize_unicode(question), normalize_unicode(answer)], model="text-embedding-004")
        if len(embeddings) != 2:
            logger.error("KhÃ´ng thá»ƒ táº¡o nhÃºng cho cÃ¢u há»i hoáº·c cÃ¢u tráº£ lá»i")
            return 0.0
        
        question_vec, answer_vec = embeddings
        cosine_sim = np.dot(question_vec, answer_vec) / (np.linalg.norm(question_vec) * np.linalg.norm(answer_vec))
        logger.info(f"Cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i - Cosine similarity: {cosine_sim:.2f}")
        
        if lang == "vi" and not check_vietnamese_spelling(answer):
            logger.warning(f"CÃ¢u tráº£ lá»i tiáº¿ng Viá»‡t khÃ´ng há»£p lá»‡: {answer}")
            return 0.0
        
        return cosine_sim
    except Exception as e:
        logger.error(f"Lá»—i kiá»ƒm tra cháº¥t lÆ°á»£ng: {e}")
        return 0.0

# HÃ m tÃ¬m kiáº¿m ngá»¯ nghÄ©a
async def semantic_search(query: str, topic: str, k: int = 30) -> List[str]:
    try:
        if topic in surtraff_details:
            return [surtraff_details[topic]]
        
        query_embedding = get_gemini_embeddings([normalize_unicode(query)], task_type="RETRIEVAL_QUERY")[0]
        if not query_embedding.size:
            logger.error("KhÃ´ng thá»ƒ táº¡o embedding cho query")
            return []
        
        faiss_results = vector_official.similarity_search_by_vector(query_embedding, k=k, filter={"topic": topic}) if vector_official else []
        if not faiss_results and topic != "General":
            faiss_results = vector_official.similarity_search_by_vector(query_embedding, k=k, filter={"topic": "General"})
        
        forbidden_terms = ["culture", "tourism", "festival"]
        filtered_docs = [normalize_unicode(r.page_content) for r in faiss_results if r.page_content.strip() and not any(term in r.page_content.lower() for term in forbidden_terms)]
        unique_docs = list(dict.fromkeys(filtered_docs))
        return unique_docs[:10]
    except Exception as e:
        logger.error(f"Lá»—i semantic search: {e}")
        return []

# HÃ m Ä‘á»‹nh dáº¡ng cÃ¢u tráº£ lá»i
async def format_response(context: str, question: str, history_summary: str, emotion: str, lang: str, parsed_info: Dict[str, str]) -> str:
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        prompt = f"""
        Dá»±a trÃªn ngá»¯ cáº£nh: {normalize_unicode(context[:800])}
        vÃ  lá»‹ch sá»­: {normalize_unicode(history_summary)}
        ThÃ´ng tin phÃ¢n tÃ­ch: Äá»™ng tá»« chÃ­nh: {parsed_info['main_verb']}, Thá»±c thá»ƒ: {', '.join(parsed_info['entities'])}, PhÆ°Æ¡ng tiá»‡n: {parsed_info['vehicle_type'] or 'khÃ´ng xÃ¡c Ä‘á»‹nh'}, Thá»i gian: {parsed_info['time'] or 'khÃ´ng xÃ¡c Ä‘á»‹nh'}, Ã Ä‘á»‹nh: {parsed_info['intent']}
        Tráº£ lá»i cÃ¢u há»i: {normalize_unicode(question)}
        Báº±ng {'tiáº¿ng Viá»‡t' if lang == 'vi' else 'English'}, ngáº¯n gá»n (tá»‘i Ä‘a 100 tá»«), Ä‘Ãºng trá»ng tÃ¢m, thÃ¢n thiá»‡n, phÃ¹ há»£p cáº£m xÃºc ({emotion}), cÃ³ emoji.
        Náº¿u khÃ´ng cÃ³ thÃ´ng tin, tráº£ lá»i 'KhÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t, há»i thÃªm nhÃ©! ğŸ˜Š'
        """
        response = model.generate_content(
            prompt,
            generation_config={"max_output_tokens": 100, "temperature": 0.7}
        ).text.strip()
        response = normalize_unicode(response)
        response = auto_correct_spelling(response)
        if lang == "vi" and not check_vietnamese_spelling(response):
            return f"KhÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t, há»i thÃªm nhÃ©! ğŸ˜Š" if lang == "vi" else f"No detailed information, ask more! ğŸ˜Š"
        return response
    except Exception as e:
        logger.error(f"Lá»—i Ä‘á»‹nh dáº¡ng cÃ¢u tráº£ lá»i: {e}")
        return f"KhÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t, há»i thÃªm nhÃ©! ğŸ˜Š" if lang == "vi" else f"No detailed information, ask more! ğŸ˜Š"

# HÃ m phÃ¢n loáº¡i cÃ¢u há»i
def classify_question_type(question: str, history: List[Dict]) -> str:
    question_lower = normalize_unicode(question.lower())
    parsed_info = parse_question(question)
    if any(keyword in question_lower for keyword in DOUBT_KEYWORDS) and history:
        return history[-1].get("type", "general")
    if parsed_info["intent"] != "unknown":
        return parsed_info["intent"]
    if any(p in question_lower for p in PLACE_NAMES) or any(k in question_lower for k in ["giao thÃ´ng", "traffic", "káº¹t xe", "traffic jam", "máº­t Ä‘á»™", "density", "tÃ¬nh tráº¡ng"]):
        return "traffic_external"
    if any(k in question_lower for k in ["hi", "hello", "chÃ o", "how are you", "báº¡n khá»e khÃ´ng"]):
        return "social"
    if "biá»ƒn sá»‘" in question_lower or extract_plate(question_lower):
        return "plate_violation"
    return "general"

# HÃ m láº¥y dá»¯ liá»‡u giao thÃ´ng tá»« web
async def fetch_external_traffic_data(query: str, lang: str, history: List[Dict]) -> str:
    cache_key = f"traffic_external:{normalize_unicode(query)}:{lang}"
    cached_response = check_similar_question(query, semantic_cache["traffic_external"])
    if cached_response:
        logger.info(f"Cache hit cho traffic query: {cache_key}")
        return cached_response
    
    try:
        location = next((place for place in PLACE_NAMES if place.lower() in normalize_unicode(query.lower())), None)
        parsed_info = parse_question(query)
        time = parsed_info["time"] or datetime.now().strftime('%d/%m/%Y')
        if not location:
            response = f"ğŸ˜Š Vui lÃ²ng chá»‰ rÃµ Ä‘á»‹a Ä‘iá»ƒm (nhÆ° HÃ  Ná»™i, Cáº§n ThÆ¡) Ä‘á»ƒ mÃ¬nh tÃ¬m thÃ´ng tin giao thÃ´ng nhÃ©!" if lang == "vi" else f"ğŸ˜Š Please specify a location (e.g., Hanoi, Can Tho) for traffic information!"
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        if location.lower() == "cáº§n thÆ¡":
            response = f"ğŸ˜Š HÃ´m nay ({time}), giao thÃ´ng á»Ÿ Cáº§n ThÆ¡ cÃ³ thá»ƒ bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi ngáº­p lá»¥t trÃªn cÃ¡c tuyáº¿n Ä‘Æ°á»ng nhÆ° Máº­u ThÃ¢n, Nguyá»…n VÄƒn Cá»« náº¿u cÃ³ mÆ°a lá»›n. ğŸ˜Š"
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        query_keywords = normalize_unicode(query.lower()).split()
        query_keywords.append(location.lower())
        search_query = " ".join(query_keywords)
        relevant_urls = [url for url in TRAFFIC_NEWS_URLS if any(kw in search_query for kw in [location.lower(), "káº¹t xe", "traffic"])] or TRAFFIC_NEWS_URLS[:2]
        
        traffic_data = []
        for url in relevant_urls:
            text = extract_text_from_url(url)
            if text and any(k in normalize_unicode(text.lower()) for k in ["giao thÃ´ng", "Ã¹n táº¯c", "káº¹t xe", "tai náº¡n"]):
                traffic_data.append(text)
        
        if not traffic_data:
            response = f"ğŸ˜” KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin giao thÃ´ng cho {location} vÃ o ngÃ y {time}." if lang == "vi" else f"ğŸ˜” No traffic information found for {location} on {time}."
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        combined_text = "\n".join(traffic_data)[:3000]
        if not combined_text.strip():
            response = f"ğŸ˜” KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin giao thÃ´ng cho {location} vÃ o ngÃ y {time}." if lang == "vi" else f"ğŸ˜” No traffic information found for {location} on {time}."
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        embeddings = get_gemini_embeddings([combined_text, query])
        if len(embeddings) != 2:
            logger.error("KhÃ´ng thá»ƒ táº¡o nhÃºng cho dá»¯ liá»‡u giao thÃ´ng")
            response = f"ğŸ˜” KhÃ´ng thá»ƒ xá»­ lÃ½ thÃ´ng tin giao thÃ´ng cho {location}." if lang == "vi" else f"ğŸ˜” Unable to process traffic information for {location}."
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        context_vec, query_vec = embeddings
        cosine_sim = np.dot(context_vec, query_vec) / (np.linalg.norm(context_vec) * np.linalg.norm(query_vec))
        
        if cosine_sim < 0.8:
            response = f"ğŸ˜” KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p cho {location} vÃ o ngÃ y {time}." if lang == "vi" else f"ğŸ˜” No relevant traffic information found for {location} on {time}."
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        model = genai.GenerativeModel("gemini-1.5-flash")
        prompt = f"TÃ³m táº¯t thÃ´ng tin giao thÃ´ng liÃªn quan Ä‘áº¿n '{normalize_unicode(query)}' tá»« dá»¯ liá»‡u sau:\n{normalize_unicode(combined_text)}\nTráº£ lá»i báº±ng {'tiáº¿ng Viá»‡t' if lang == 'vi' else 'English'}, ngáº¯n gá»n, tá»‘i Ä‘a 100 tá»«, cÃ³ emoji."
        response = model.generate_content(
            prompt,
            generation_config={"max_output_tokens": 100, "temperature": 0.7}
        ).text.strip()
        response = normalize_unicode(response)
        response = auto_correct_spelling(response)
        
        if not check_vietnamese_spelling(response) and lang == "vi":
            response = f"ğŸ˜Š KhÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t vá» {location} vÃ o ngÃ y {time}. Há»i thÃªm vá» giao thÃ´ng nhÃ©!" if lang == "vi" else f"ğŸ˜Š No detailed information for {location} on {time}. Ask more about traffic!"
        
        semantic_cache["traffic_external"][cache_key] = response
        return response
    except Exception as e:
        logger.error(f"Lá»—i fetch_external_traffic_data: {e}")
        response = f"ğŸ˜” CÃ³ lá»—i khi láº¥y thÃ´ng tin giao thÃ´ng cho {location}. Vui lÃ²ng thá»­ láº¡i!" if lang == "vi" else f"ğŸ˜” Error fetching traffic information for {location}. Please try again!"
        semantic_cache["traffic_external"][cache_key] = response
        return response

# HÃ m láº¥y dá»¯ liá»‡u vi pháº¡m tá»« API
@retry_decorator(tries=4, delay=1, backoff=2)
async def fetch_violation_data(plate: str = None, location: str = None, lang: str = "vi") -> str:
    cache_key = f"violation:{plate}:{location}:{lang}"
    cached_response = check_similar_question(f"{plate}:{location}", semantic_cache["plate_violation"])
    if cached_response:
        logger.info(f"Cache hit cho violation query: {cache_key}")
        return cached_response
    
    try:
        api_url = "http://localhost:8081/api/violations"
        headers = {"Content-Type": "application/json"}
        response = requests.get(api_url, headers=headers, timeout=15)
        if response.status_code != 200:
            logger.error(f"API tráº£ vá» mÃ£ lá»—i: {response.status_code}")
            response_text = f"ğŸ˜” Lá»—i API, khÃ´ng thá»ƒ kiá»ƒm tra vi pháº¡m. Thá»­ láº¡i sau!" if lang == "vi" else f"ğŸ˜” API error, cannot check violations. Try again later!"
            semantic_cache["plate_violation"][cache_key] = response_text
            return response_text
        data = response.json()
        
        if not data:
            response_text = f"âœ… KhÃ´ng tÃ¬m tháº¥y vi pháº¡m cho {'biá»ƒn sá»‘ ' + plate if plate else 'khu vá»±c ' + location} vÃ o ngÃ y {datetime.now().strftime('%d/%m/%Y')}." if lang == "vi" else f"âœ… No violations found for {'license plate ' + plate if plate else 'location ' + location} on {datetime.now().strftime('%d/%m/%Y')}."
            semantic_cache["plate_violation"][cache_key] = response_text
            return response_text
        
        violations = []
        for item in data:
            vehicle_plate = item.get('vehicle', {}).get('licensePlate')
            item_location = item.get('camera', {}).get('location')
            if not vehicle_plate or not item_location:
                continue
            vehicle_plate = vehicle_plate.upper()
            item_location = item_location.upper()
            if (plate and vehicle_plate != plate.upper()) or (location and item_location != location.upper()):
                continue
            for detail in item.get("violationDetails", []):
                violation_time = datetime.strptime(detail.get('violationTime', ''), '%Y-%m-%dT%H:%M:%S').strftime('%H:%M %d/%m/%Y') if detail.get('violationTime') else "KhÃ´ng xÃ¡c Ä‘á»‹nh"
                violation_type = detail.get('violationType', {}).get('typeName', 'KhÃ´ng xÃ¡c Ä‘á»‹nh')
                location = detail.get('location', 'KhÃ´ng xÃ¡c Ä‘á»‹nh')
                additional_notes = detail.get('additionalNotes', 'KhÃ´ng cÃ³ ghi chÃº')
                status = item.get('status', 'KhÃ´ng xÃ¡c Ä‘á»‹nh').lower()
                violations.append(
                    f"- {violation_type.capitalize()} lÃºc {violation_time} táº¡i {location} ({additional_notes}, tráº¡ng thÃ¡i: {status})"
                )
        
        if not violations:
            response_text = f"âœ… KhÃ´ng tÃ¬m tháº¥y vi pháº¡m cho {'biá»ƒn sá»‘ ' + plate if plate else 'khu vá»±c ' + location} vÃ o ngÃ y {datetime.now().strftime('%d/%m/%Y')}." if lang == "vi" else f"âœ… No violations found for {'license plate ' + plate if plate else 'location ' + location} on {datetime.now().strftime('%d/%m/%Y')}."
        else:
            response_text = f"ğŸš¨ {'Biá»ƒn sá»‘ ' + plate if plate else 'Khu vá»±c ' + location} cÃ³ vi pháº¡m:\n" + "\n".join(violations)
        
        response_text = normalize_unicode(response_text)
        semantic_cache["plate_violation"][cache_key] = response_text
        return response_text
    except requests.exceptions.RequestException as e:
        logger.error(f"Lá»—i gá»i API: {e}")
        response = f"ğŸ˜” Lá»—i há»‡ thá»‘ng, khÃ´ng thá»ƒ kiá»ƒm tra vi pháº¡m!" if lang == "vi" else f"ğŸ˜” System error, cannot check violations!"
        semantic_cache["plate_violation"][cache_key] = response
        return response

# HÃ m táº£i chunks tá»« pháº£n há»“i
def load_feedback_chunks() -> tuple:
    chunks = []
    metadata = []
    try:
        if not os.path.exists(FEEDBACK_FILE):
            return [], []
        with open(FEEDBACK_FILE, "r", encoding="utf-8-sig") as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    entry = json.loads(line)
                    content = normalize_unicode(entry.get("content", ""))
                    question = normalize_unicode(entry.get("question", ""))
                    topic = entry.get("topic", "General")
                    if content and question:
                        chunk = f"{question} {content}"
                        chunks.append(chunk)
                        metadata.append({"topic": topic, "source": "feedback"})
                except json.JSONDecodeError:
                    logger.warning(f"JSON khÃ´ng há»£p lá»‡ trong feedback: {line.strip()}")
                    continue
        return chunks, metadata
    except Exception as e:
        logger.error(f"Lá»—i táº£i feedback chunks: {e}")
        return [], []

# HÃ m cáº­p nháº­t FAISS index ngÆ°á»i dÃ¹ng
async def update_user_index():
    global vector_user
    user_index_path = FAISS_INDEX_PATH + "_user.pkl"
    
    if not check_system_resources() or not check_disk_space(os.path.dirname(FAISS_INDEX_PATH), 1000):
        logger.error("TÃ i nguyÃªn há»‡ thá»‘ng khÃ´ng Ä‘á»§ Ä‘á»ƒ cáº­p nháº­t FAISS index")
    
    chunks, metadata = load_feedback_chunks()
    if not chunks:
        logger.info("KhÃ´ng cÃ³ dá»¯ liá»‡u pháº£n há»“i Ä‘á»ƒ cáº­p nháº­t index")
        return
    
    try:
        embeddings = get_gemini_embeddings(chunks, task_type="RETRIEVAL_DOCUMENT")
        if not embeddings:
            logger.error("KhÃ´ng thá»ƒ táº¡o embeddings tá»« Gemini API")
            return
        
        dimension = len(embeddings[0])
        index = faiss.IndexHNSWFlat(dimension, 32)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 40
        index.add(np.array(embeddings))
        vector_user = FAISS.from_vectors(vectors=embeddings, texts=chunks, metadatas=metadata, faiss_index=index)
        
        with open(user_index_path, "wb") as f:
            import pickle
            pickle.dump(vector_user, f)
        logger.info("ÄÃ£ cáº­p nháº­t FAISS user index (sá»­ dá»¥ng CPU)")
    except Exception as e:
        logger.error(f"Lá»—i cáº­p nháº­t FAISS user index: {e}")

# HÃ m táº¡o vector FAISS
def build_vector_official():
    global surtraff_details
    official_index_path = FAISS_INDEX_PATH + "_official.pkl"
    
    if not check_system_resources() or not check_disk_space(os.path.dirname(FAISS_INDEX_PATH), 1000):
        logger.error("TÃ i nguyÃªn há»‡ thá»‘ng khÃ´ng Ä‘á»§ Ä‘á»ƒ táº¡o FAISS index")
    
    try:
        if os.path.exists(official_index_path) and os.path.getsize(official_index_path) > 0:
            with open(official_index_path, "rb") as f:
                import pickle
                store = pickle.load(f)
            logger.info("ÄÃ£ táº£i FAISS official index")
            return store
    except Exception as e:
        logger.error(f"Lá»—i táº£i FAISS official index: {e}, táº¡o má»›i index")
    
    surtraff_text = extract_text_from_txt(KNOWLEDGE_TXT_PATH, prioritize_dialogs=True)
    social_text = extract_text_from_txt(SOCIAL_TXT_PATH)
    combined_text = f"{surtraff_text}\n{social_text}"
    if not combined_text.strip():
        logger.error("File surtraff_knowledge.txt, traffic_dialogs.txt hoáº·c social.txt rá»—ng")
        return None
    
    surtraff_chunks = text_splitter.split_text(combined_text)
    if not surtraff_chunks:
        logger.error("KhÃ´ng cÃ³ dá»¯ liá»‡u trong surtraff_chunks")
        return None
    
    surtraff_chunks = list(dict.fromkeys(surtraff_chunks))
    surtraff_metadata = [{"topic": detect_topic(chunk), "source": "surtraff"} for chunk in surtraff_chunks]
    
    try:
        forbidden_terms = ["culture", "tourism", "festival"]
        filtered_chunks = [c for c in surtraff_chunks if c.strip() and not any(term in normalize_unicode(c.lower()) for term in forbidden_terms)]
        embeddings = get_gemini_embeddings(texts=filtered_chunks, task_type="RETRIEVAL_DOCUMENT", output_dimensionality=512)
        if not embeddings:
            logger.error("KhÃ´ng thá»ƒ táº¡o embeddings tá»« Gemini API")
            return None
        
        dimension = len(embeddings[0])
        index = faiss.IndexHNSWFlat(dimension, 32)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 40
        index.add(np.array(embeddings))
        store = FAISS.from_vectors(vectors=embeddings, texts=filtered_chunks, metadatas=[m for c, m in zip(surtraff_chunks, surtraff_metadata) if c.strip() and not any(term in normalize_unicode(c.lower()) for term in forbidden_terms)], faiss_index=index)
        
        with open(official_index_path, "wb") as f:
            import pickle
            pickle.dump(store, f)
        logger.info("ÄÃ£ táº¡o vÃ  lÆ°u FAISS official index (sá»­ dá»¥ng CPU)")
        return store
    except Exception as e:
        logger.error(f"Lá»—i táº¡o FAISS index: {e}")
        return None

def build_vector_user():
    global vector_user
    user_index_path = FAISS_INDEX_PATH + "_user.pkl"
    
    if not check_system_resources() or not check_disk_space(os.path.dirname(FAISS_INDEX_PATH), 1000):
        logger.error("TÃ i nguyÃªn há»‡ thá»‘ng khÃ´ng Ä‘á»§ Ä‘á»ƒ táº¡o FAISS user index")
        
    
    try:
        if os.path.exists(user_index_path) and os.path.getsize(user_index_path) > 0:
            with open(user_index_path, "rb") as f:
                import pickle
                store = pickle.load(f)
            logger.info("ÄÃ£ táº£i FAISS user index")
            return store
    except Exception as e:
        logger.error(f"Lá»—i táº£i FAISS user index: {e}, táº¡o má»›i index")
    
    chunks, metadata = load_feedback_chunks()
    if not chunks:
        logger.info("KhÃ´ng cÃ³ dá»¯ liá»‡u pháº£n há»“i Ä‘á»ƒ táº¡o index")
        return None
    
    try:
        embeddings = get_gemini_embeddings(chunks, task_type="RETRIEVAL_DOCUMENT")
        if not embeddings:
            logger.error("KhÃ´ng thá»ƒ táº¡o embeddings tá»« Gemini API")
            return None
        
        dimension = len(embeddings[0])
        index = faiss.IndexHNSWFlat(dimension, 32)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 40
        index.add(np.array(embeddings))
        vector_user = FAISS.from_vectors(vectors=embeddings, texts=chunks, metadatas=metadata, faiss_index=index)
        
        with open(user_index_path, "wb") as f:
            import pickle
            pickle.dump(vector_user, f)
        logger.info("ÄÃ£ táº¡o vÃ  lÆ°u FAISS user index (sá»­ dá»¥ng CPU)")
        return vector_user
    except Exception as e:
        logger.error(f"Lá»—i táº¡o FAISS user index: {e}")
        return None

vector_official = build_vector_official()
vector_user = build_vector_user()

# HÃ m chÃ­nh xá»­ lÃ½ cÃ¢u há»i
async def process_question(question: str, history: List[Dict] = []) -> Dict[str, str]:
    try:
        if not question.strip() or not is_safe_input(question):
            return {"response": "CÃ¢u há»i khÃ´ng há»£p lá»‡, vui lÃ²ng thá»­ láº¡i! ğŸ˜Š", "lang": "vi", "suggestion": "", "type": "general"}

        cleaned_question = clean_question(question)
        if not cleaned_question:
            return {"response": "CÃ¢u há»i khÃ´ng há»£p lá»‡, vui lÃ²ng thá»­ láº¡i! ğŸ˜Š", "lang": "vi", "suggestion": "", "type": "general"}

        lang = await detect_language(cleaned_question, history)
        question_type = classify_question_type(cleaned_question, history)
        parsed_info = parse_question(cleaned_question)
        time_of_day = get_time_of_day()
        emotion = detect_emotion(cleaned_question)
        history_summary = await summarize_context(history)
        plate = extract_plate(cleaned_question)
        location = next((place for place in PLACE_NAMES if place.lower() in normalize_unicode(cleaned_question.lower())), None)

        cache_key = f"{question_type}:{normalize_unicode(cleaned_question)}:{lang}"
        cached_response = check_similar_question(cleaned_question, semantic_cache.get(question_type, semantic_cache["general"]))
        if cached_response:
            logger.info(f"Cache hit cho cÃ¢u há»i: {cache_key}")
            suggestion = await generate_suggested_questions(history, question_type, lang)
            return {"response": cached_response, "lang": lang, "suggestion": suggestion, "type": question_type}

        if question_type == "social":
            response = await get_social_response(cleaned_question, lang, time_of_day, history, emotion)
            semantic_cache["social"][cache_key] = response
            suggestion = await generate_suggested_questions(history, question_type, lang)
            log_chat(cleaned_question, response, lang, question_type)
            save_feedback(cleaned_question, response, lang)
            return {"response": response, "lang": lang, "suggestion": suggestion, "type": question_type}

        if question_type == "plate_violation" and plate:
            response = await fetch_violation_data(plate=plate, location=location, lang=lang)
            suggestion = await generate_suggested_questions(history, question_type, lang)
            log_chat(cleaned_question, response, lang, question_type, plate)
            save_feedback(cleaned_question, response, lang)
            return {"response": response, "lang": lang, "suggestion": suggestion, "type": question_type}

        if question_type == "traffic_external" and location:
            response = await fetch_external_traffic_data(cleaned_question, lang, history)
            suggestion = await generate_suggested_questions(history, question_type, lang)
            log_chat(cleaned_question, response, lang, question_type)
            save_feedback(cleaned_question, response, lang)
            return {"response": response, "lang": lang, "suggestion": suggestion, "type": question_type}

        context_docs = await semantic_search(cleaned_question, question_type)
        context = "\n".join(context_docs) if context_docs else surtraff_details.get(question_type, "")
        if not context.strip():
            context = surtraff_details.get("General", "KhÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t.")

        response = await format_response(context, cleaned_question, history_summary, emotion, lang, parsed_info)
        quality_score = await check_answer_quality(cleaned_question, response, lang)
        if quality_score < 0.7:
            logger.warning(f"Cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i tháº¥p ({quality_score:.2f}), thá»­ láº¡i vá»›i Gemini")
            try:
                model = genai.GenerativeModel("gemini-1.5-flash")
                prompt = f"""
                Tráº£ lá»i cÃ¢u há»i: {normalize_unicode(cleaned_question)}
                Dá»±a trÃªn ngá»¯ cáº£nh: {normalize_unicode(context[:800])}
                Ngá»¯ cáº£nh lá»‹ch sá»­: {normalize_unicode(history_summary)}
                ThÃ´ng tin phÃ¢n tÃ­ch: Äá»™ng tá»« chÃ­nh: {parsed_info['main_verb']}, Thá»±c thá»ƒ: {', '.join(parsed_info['entities'])}, PhÆ°Æ¡ng tiá»‡n: {parsed_info['vehicle_type'] or 'khÃ´ng xÃ¡c Ä‘á»‹nh'}, Thá»i gian: {parsed_info['time'] or 'khÃ´ng xÃ¡c Ä‘á»‹nh'}, Ã Ä‘á»‹nh: {parsed_info['intent']}
                Báº±ng {'tiáº¿ng Viá»‡t' if lang == 'vi' else 'English'}, ngáº¯n gá»n (tá»‘i Ä‘a 100 tá»«), thÃ¢n thiá»‡n, phÃ¹ há»£p cáº£m xÃºc ({emotion}), cÃ³ emoji.
                Náº¿u khÃ´ng cÃ³ thÃ´ng tin, tráº£ lá»i 'KhÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t, há»i thÃªm nhÃ©! ğŸ˜Š'
                """
                response = model.generate_content(
                    prompt,
                    generation_config={"max_output_tokens": 100, "temperature": 0.7}
                ).text.strip()
                response = normalize_unicode(response)
                response = auto_correct_spelling(response)
                quality_score = await check_answer_quality(cleaned_question, response, lang)
                if quality_score < 0.5:
                    response = f"KhÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t, há»i thÃªm nhÃ©! ğŸ˜Š" if lang == "vi" else f"No detailed information, ask more! ğŸ˜Š"
            except Exception as e:
                logger.error(f"Lá»—i gá»i Gemini API: {e}")
                response = f"KhÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t, há»i thÃªm nhÃ©! ğŸ˜Š" if lang == "vi" else f"No detailed information, ask more! ğŸ˜Š"

        semantic_cache[question_type][cache_key] = response
        suggestion = await generate_suggested_questions(history, question_type, lang)
        log_chat(cleaned_question, response, lang, question_type, plate)
        save_feedback(cleaned_question, response, lang)
        return {"response": response, "lang": lang, "suggestion": suggestion, "type": question_type}

    except Exception as e:
        logger.error(f"Lá»—i xá»­ lÃ½ cÃ¢u há»i: {e}")
        response = f"ğŸ˜” CÃ³ lá»—i xáº£y ra, vui lÃ²ng thá»­ láº¡i!" if lang == "vi" else f"ğŸ˜” An error occurred, please try again!"
        suggestion = await generate_suggested_questions(history, "general", lang)
        log_chat(cleaned_question, response, lang, "error", plate)
        return {"response": response, "lang": lang, "suggestion": suggestion, "type": "error"}

# HÃ m xá»­ lÃ½ cÃ¢u há»i Ä‘á»“ng bá»™ (cho mÃ´i trÆ°á»ng khÃ´ng async)
def process_question_sync(question: str, history: List[Dict] = []) -> Dict[str, str]:
    return asyncio.run(process_question(question, history))

# HÃ m cháº¡y chatbot trong cháº¿ Ä‘á»™ tÆ°Æ¡ng tÃ¡c
async def run_chatbot():
    history = []
    print("ChÃ o báº¡n! Há»i mÃ¬nh vá» giao thÃ´ng hoáº·c SurTraff nhÃ©! ğŸ˜Š (Nháº­p 'exit' Ä‘á»ƒ thoÃ¡t)")
    while True:
        question = input("CÃ¢u há»i cá»§a báº¡n: ")
        if question.lower() == "exit":
            print("Táº¡m biá»‡t! ğŸ˜Š")
            break
        result = await process_question(question, history)
        print(f"Tráº£ lá»i: {result['response']}")
        if result['suggestion']:
            print(f"{result['suggestion']}")
        history.append({
            "sentence": normalize_unicode(question),
            "response": normalize_unicode(result['response']),
            "lang": result['lang'],
            "type": result['type']
        })
        history = history[-5:]  # Giá»¯ tá»‘i Ä‘a 5 cÃ¢u há»i gáº§n nháº¥t

# HÃ m cháº¡y chatbot Ä‘á»“ng bá»™
def run_chatbot_sync():
    asyncio.run(run_chatbot())

if __name__ == "__main__":
    run_chatbot_sync()
