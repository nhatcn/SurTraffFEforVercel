import os
import re
import json
import random
import requests
from datetime import datetime
from cachetools import TTLCache
import logging
from bs4 import BeautifulSoup
from fuzzywuzzy import fuzz
import numpy as np
import asyncio
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
import faiss
import google.generativeai as genai
from google.api_core import retry
import psutil
from retry import retry as retry_decorator
import spacy
from typing import List, Dict, Optional
import unicodedata
from langchain_core.embeddings import Embeddings

# Thi·∫øt l·∫≠p logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n file
KNOWLEDGE_TXT_PATH = "services/chatbot/surtraff_knowledge.txt"
SOCIAL_TXT_PATH = "services/chatbot/social.txt"
TRAFFIC_DIALOGS_PATH = "services/chatbot/traffic_dialogs.txt"
FEEDBACK_FILE = "services/chatbot/custom_knowledge.jsonl"
FAISS_INDEX_PATH = "services/chatbot/faiss_index"
CHAT_LOG_FILE = "services/chatbot/chat_log.jsonl"
LIMIT_FEEDBACK = 1000

# Thi·∫øt l·∫≠p API key
API_KEY = os.getenv("GOOGLE_API_KEY", "")
genai.configure(api_key=API_KEY)

# T·∫£i m√¥ h√¨nh spaCy cho ti·∫øng Vi·ªát
try:
    nlp_vi = spacy.load("vi_core_news_md")
except:
    logger.warning("Kh√¥ng t·∫£i ƒë∆∞·ª£c spaCy model vi_core_news_md, b·ªè qua ph√¢n t√≠ch c√∫ ph√°p")
    nlp_vi = None

# S·ª≠ d·ª•ng FAISS-CPU m·∫∑c ƒë·ªãnh
logger.info("S·ª≠ d·ª•ng FAISS-CPU m·∫∑c ƒë·ªãnh.")

# Danh s√°ch ƒë·ªãa danh
PLACE_NAMES = [
    "H√† N·ªôi", "H·ªì Ch√≠ Minh", "S√†i G√≤n", "ƒê√† N·∫µng", "H·∫£i Ph√≤ng", "C·∫ßn Th∆°", "Hu·∫ø", "Nha Trang",
    "V≈©ng T√†u", "ƒê√† L·∫°t", "B√¨nh D∆∞∆°ng", "ƒê·ªìng Nai", "Kh√°nh H√≤a", "Qu·∫£ng Ninh", "C√† Mau",
    "An Giang", "B√† R·ªãa-V≈©ng T√†u", "B·∫Øc Giang", "B·∫Øc K·∫°n", "B·∫°c Li√™u", "B·∫Øc Ninh", "B·∫øn Tre",
    "B√¨nh ƒê·ªãnh", "B√¨nh Ph∆∞·ªõc", "B√¨nh Thu·∫≠n", "Cao B·∫±ng", "ƒê·∫Øk L·∫Øk", "ƒê·∫Øk N√¥ng", "ƒêi·ªán Bi√™n",
    "ƒê·ªìng Th√°p", "Gia Lai", "H√† Giang", "H√† Nam", "H√† Tƒ©nh", "H·∫£i D∆∞∆°ng", "H·∫≠u Giang",
    "H√≤a B√¨nh", "H∆∞ng Y√™n", "Ki√™n Giang", "Kon Tum", "Lai Ch√¢u", "L√¢m ƒê·ªìng", "L·∫°ng S∆°n",
    "L√†o Cai", "Long An", "Nam ƒê·ªãnh", "Ngh·ªá An", "Ninh B√¨nh", "Ninh Thu·∫≠n", "Ph√∫ Th·ªç",
    "Ph√∫ Y√™n", "Qu·∫£ng B√¨nh", "Qu·∫£ng Nam", "Qu·∫£ng Ng√£i", "Qu·∫£ng Tr·ªã", "S√≥c TrƒÉng",
    "S∆°n La", "T√¢y Ninh", "Th√°i B√¨nh", "Th√°i Nguy√™n", "Thanh H√≥a", "Ti·ªÅn Giang",
    "Tr√† Vinh", "Tuy√™n Quang", "Vƒ©nh Long", "Vƒ©nh Ph√∫c", "Y√™n B√°i"
]

# B·ªô nh·ªõ cache ph√¢n v√πng
translation_cache = TTLCache(maxsize=1000, ttl=43200)  # 12 gi·ªù
feedback_cache = TTLCache(maxsize=1000, ttl=86400)     # 1 ng√†y
semantic_cache = {
    "traffic_law": TTLCache(maxsize=500, ttl=604800),
    "plate_violation": TTLCache(maxsize=500, ttl=3600),
    "traffic_external": TTLCache(maxsize=500, ttl=3600),
    "method_violation": TTLCache(maxsize=500, ttl=3600),
    "social": TTLCache(maxsize=200, ttl=86400),
    "general": TTLCache(maxsize=500, ttl=604800)
}
web_cache = TTLCache(maxsize=100, ttl=3600)  # 1 gi·ªù

# T·ª´ kh√≥a nghi ng·ªù
DOUBT_KEYWORDS = ["thi·ªát", "ch·∫Øc", "th·∫≠t", "c√≥ ch·∫Øc", "really", "sure", "is it true"]

# T·ª´ ƒëi·ªÉn d·ªãch thu·∫≠t
keyword_map = {
    "v∆∞·ª£t ƒë√®n ƒë·ªè": "red light violation",
    "giao th√¥ng": "traffic",
    "m≈© b·∫£o hi·ªÉm": "helmet",
    "tai n·∫°n": "accident",
    "t·ªëc ƒë·ªô": "speed",
    "ƒë√®n ƒë·ªè": "red light",
    "bi·ªÉn s·ªë": "license plate",
    "ƒë·ªó xe sai": "illegal parking",
    "ch·∫°y sai l√†n": "lane violation",
    "ng∆∞·ª£c chi·ªÅu": "wrong-way driving",
    "v·∫≠t c·∫£n": "obstacle",
    "h·ªë tr√™n ƒë∆∞·ªùng": "pothole",
    "m·∫≠t ƒë·ªô xe": "traffic density",
    "√πn t·∫Øc giao th√¥ng": "traffic jam",
    "quy ƒë·ªãnh giao th√¥ng": "traffic regulation",
    "m·ª©c ph·∫°t": "fine",
    "vi ph·∫°m giao th√¥ng": "traffic violation",
    "ƒë∆∞·ªùng cao t·ªëc": "highway",
    "camera giao th√¥ng": "traffic camera",
    "k·∫πt xe": "traffic jam",
    "b·∫±ng l√°i": "driver's license",
    "ƒë√®n giao th√¥ng": "traffic light",
    "ƒë∆∞·ªùng m·ªôt chi·ªÅu": "one-way road",
    "ph√¢n lu·ªìng giao th√¥ng": "traffic diversion",
    "c·∫£nh s√°t giao th√¥ng": "traffic police",
    "ƒë∆∞·ªùng qu·ªëc l·ªô": "national highway",
    "ƒë∆∞·ªùng t·ªânh l·ªô": "provincial road",
    "ƒëƒÉng ki·ªÉm": "vehicle inspection",
    "x·ª≠ ph·∫°t": "penalize",
    "th·ªùi gian th·ª±c": "real-time",
    "h√†nh vi": "behavior",
    "b√°o c√°o": "report",
    "b·∫£n ƒë·ªì": "map",
    "an to√†n": "safety",
    "gi·ªõi h·∫°n": "limit",
    "t√≠n hi·ªáu": "signal",
    "ƒë∆∞·ªùng ph·ªë": "street",
    "ph√¢n t√≠ch": "analysis",
    "yolo": "YOLO",
    "camera": "camera",
    "nh·∫≠n di·ªán": "recognition",
    "h·ªá th·ªëng": "system",
    "ng√£ t∆∞": "intersection",
    "ng√£ ba": "T-junction",
    "c·∫ßu": "bridge",
    "h·∫ßm": "tunnel",
    "ƒëo·∫°n ƒë∆∞·ªùng": "road segment",
    "khu v·ª±c": "area",
    "v·ªã tr√≠": "location",
    "h∆∞·ªõng": "direction",
    "b√°o l·ªói h·ªá th·ªëng": "report system error",
    "ƒëƒÉng nh·∫≠p": "login",
    "nh∆∞ th·∫ø n√†o": "how",
    "surtraff": "SurTraff"
}

# T·ª´ ƒëi·ªÉn ki·ªÉm tra ch√≠nh t·∫£
valid_vietnamese_words = set([
    "giao th√¥ng", "v∆∞·ª£t ƒë√®n ƒë·ªè", "m≈© b·∫£o hi·ªÉm", "tai n·∫°n", "t·ªëc ƒë·ªô", "ƒë√®n ƒë·ªè", "bi·ªÉn s·ªë", "ƒë·ªó xe",
    "sai l√†n", "ng∆∞·ª£c chi·ªÅu", "v·∫≠t c·∫£n", "h·ªë", "m·∫≠t ƒë·ªô", "√πn t·∫Øc", "ph·∫°t", "ngh·ªã ƒë·ªãnh", "lu·∫≠t",
    "quy ƒë·ªãnh", "yolo", "camera", "ph√°t hi·ªán", "nh·∫≠n di·ªán", "k·∫πt xe", "b·∫±ng l√°i", "ƒë√®n giao th√¥ng",
    "ƒë∆∞·ªùng m·ªôt chi·ªÅu", "ph√¢n lu·ªìng", "c·∫£nh s√°t", "qu·ªëc l·ªô", "t·ªânh l·ªô", "ƒëƒÉng ki·ªÉm", "vi ph·∫°m",
    "th·ªùi gian th·ª±c", "ph√¢n t√≠ch", "h√†nh vi", "b√°o c√°o", "b·∫£n ƒë·ªì", "h·ªá th·ªëng", "surtraff",
    "ƒë∆∞·ªùng ph·ªë", "an to√†n", "gi·ªõi h·∫°n", "t√≠n hi·ªáu", "x·ª≠ ph·∫°t", "ng√£ t∆∞", "ng√£ ba", "c·∫ßu", "h·∫ßm",
    "ƒëo·∫°n ƒë∆∞·ªùng", "khu v·ª±c", "v·ªã tr√≠", "h∆∞·ªõng", "b√°o l·ªói", "ki·ªÉm tra", "truy v·∫•n", "ƒëƒÉng nh·∫≠p",
    "nh∆∞ th·∫ø n√†o", "l√†m sao", "c√¥ng ngh·ªá", "bi·∫øt", "t√¨nh tr·∫°ng", "th√¥ng", "h√¥m nay", "th·∫ø n√†o",
    "ph∆∞∆°ng ti·ªán", "xe m√°y", "√¥ t√¥", "xe t·∫£i", "xe kh√°ch", "th·ªùi gian", "ng√†y", "th√°ng", "nƒÉm",
    "ph√°t tri·ªÉn", "t√¨nh tr·∫°ng", "h·ªèi", "tr·∫£ l·ªùi", "h·ªó tr·ª£", "ti·∫øp t·ª•c", "th√™m", "li√™n quan"
])

# Danh s√°ch URL tin t·ª©c giao th√¥ng
TRAFFIC_NEWS_URLS = [
    "https://vnexpress.net/giao-thong",
    "https://thanhnien.vn/giao-thong.htm",
    "https://tuoitre.vn/giao-thong.htm",
    "https://nld.com.vn/giao-thong.htm",
    "https://zingnews.vn/giao-thong.html",
    "https://baocantho.com.vn/"
]

from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

def init_faiss_index():
    global vector_official
    try:
        logger.info("Kh·ªüi t·∫°o FAISS index...")
        embedding_model = GeminiEmbeddings()

        chunks, metadata = load_feedback_chunks()

        # N·∫øu kh√¥ng c√≥ feedback, d√πng d·ªØ li·ªáu c·ª©ng
        if not chunks:
            logger.warning("Kh√¥ng c√≥ feedback, d√πng d·ªØ li·ªáu c·ª©ng ƒë·ªÉ t·∫°o FAISS index.")

            # 1. T·ª´ surtraff_details
            for key, val in surtraff_details.items():
                if val.strip():
                    chunks.append(val)
                    metadata.append({"topic": key})

            # 2. T·ª´ surtraff_knowledge.txt
            if os.path.exists(KNOWLEDGE_TXT_PATH):
                knowledge_text = extract_text_from_txt(KNOWLEDGE_TXT_PATH)
                if knowledge_text.strip():
                    chunks.append(knowledge_text)
                    metadata.append({"topic": "General"})

            # 3. T·ª´ traffic_dialogs.txt
            if os.path.exists(TRAFFIC_DIALOGS_PATH) and validate_jsonl_file(TRAFFIC_DIALOGS_PATH):
                with open(TRAFFIC_DIALOGS_PATH, "r", encoding="utf-8-sig") as f:
                    for line in f:
                        if not line.strip():
                            continue
                        try:
                            entry = json.loads(line)
                            q = entry.get("question", "")
                            a = " ".join(entry.get("answers", []))
                            if q and a:
                                chunks.append(f"{q} {a}")
                                metadata.append({"topic": detect_topic(q)})
                        except json.JSONDecodeError:
                            continue

            if not chunks:
                logger.error("Kh√¥ng c√≥ d·ªØ li·ªáu c·ª©ng n√†o ƒë·ªÉ t·∫°o FAISS index.")
                return

        # Build FAISS index
        docs = [Document(page_content=chunk, metadata=meta) for chunk, meta in zip(chunks, metadata)]
        texts = text_splitter.split_documents(docs)
        vector_official = FAISS.from_documents(texts, embedding_model)
        vector_official.save_local(FAISS_INDEX_PATH)
        logger.info("ƒê√£ t·∫°o v√† l∆∞u FAISS index th√†nh c√¥ng.")
    except Exception as e:
        logger.error(f"L·ªói kh·ªüi t·∫°o FAISS index: {e}")

def load_faiss_index():
    global vector_official
    try:
        logger.info("ƒêang load FAISS index t·ª´ ·ªï ƒëƒ©a...")
        embedding_model = GeminiEmbeddings()
        vector_official = FAISS.load_local(FAISS_INDEX_PATH, embeddings=embedding_model, index_name="index")
        logger.info("ƒê√£ load FAISS index th√†nh c√¥ng.")
    except Exception as e:
        logger.warning(f"Kh√¥ng th·ªÉ load FAISS index: {e}")
        vector_official = None


# Ki·ªÉm tra t√†i nguy√™n h·ªá th·ªëng
def check_system_resources():
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        logger.info(f"CPU usage: {cpu_percent}%, Memory usage: {memory_percent}%")
        return cpu_percent <= 85 and memory_percent <= 85
    except Exception as e:
        logger.error(f"L·ªói ki·ªÉm tra t√†i nguy√™n: {e}")
        return False

# Ki·ªÉm tra dung l∆∞·ª£ng ƒëƒ©a
def check_disk_space(path: str, required_mb: int) -> bool:
    try:
        disk = psutil.disk_usage(path)
        free_mb = disk.free / (1024 ** 2)
        logger.info(f"Dung l∆∞·ª£ng tr·ªëng t·∫°i {path}: {free_mb:.2f} MB")
        return free_mb >= required_mb
    except Exception as e:
        logger.error(f"L·ªói ki·ªÉm tra dung l∆∞·ª£ng ƒëƒ©a: {e}")
        return False

# Ki·ªÉm tra v√† t·∫°o file ph·∫£n h·ªìi v√† log
for file_path in [FEEDBACK_FILE, CHAT_LOG_FILE]:
    if not os.path.exists(file_path):
        with open(file_path, "a", encoding="utf-8") as f:
            pass
        logger.info(f"ƒê√£ t·∫°o file {file_path}")

# Ki·ªÉm tra file ƒë·∫ßu v√†o
for path in [KNOWLEDGE_TXT_PATH, SOCIAL_TXT_PATH, TRAFFIC_DIALOGS_PATH]:
    if not os.path.exists(path):
        logger.warning(f"File kh√¥ng t·ªìn t·∫°i: {path}")
    else:
        try:
            with open(path, "r", encoding="utf-8-sig") as f:
                content = f.read()
                logger.info(f"File {path}: {len(content)} k√Ω t·ª±")
        except Exception as e:
            logger.error(f"L·ªói ƒë·ªçc file {path}: {e}")

# Ki·ªÉm tra JSONL
def validate_jsonl_file(path: str) -> bool:
    if not os.path.exists(path):
        logger.warning(f"File kh√¥ng t·ªìn t·∫°i: {path}")
        return False
    try:
        with open(path, "r", encoding="utf-8-sig") as file:
            for i, line in enumerate(file, 1):
                if not line.strip():
                    continue
                try:
                    json.loads(line)
                except json.JSONDecodeError:
                    logger.error(f"D√≤ng {i} trong {path} kh√¥ng h·ª£p l·ªá: {line.strip()}")
                    return False
        logger.info(f"File {path} h·ª£p l·ªá")
        return True
    except Exception as e:
        logger.error(f"L·ªói ki·ªÉm tra {path}: {e}")
        return False

# H√†m ki·ªÉm tra ƒë·∫ßu v√†o an to√†n
def is_safe_input(text: str) -> bool:
    dangerous_patterns = [
        r'<\s*script', r'javascript:', r'sqlmap', r'\bselect\s+.*\s+from\b',
        r'--\s*', r';\s*drop\s+', r';\s*delete\s+', r'\bunion\s+select\b'
    ]
    for pattern in dangerous_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            logger.warning(f"Ph√°t hi·ªán ƒë·∫ßu v√†o nguy hi·ªÉm: {text}")
            return False
    return True

# H√†m chu·∫©n h√≥a Unicode
def normalize_unicode(text: str) -> str:
    return unicodedata.normalize('NFC', text)

# H√†m ki·ªÉm tra c√¢u h·ªèi t∆∞∆°ng t·ª± trong cache
def check_similar_question(question: str, cache: TTLCache) -> Optional[str]:
    for key in cache.keys():
        cached_question = key.split(":")[-1]
        if fuzz.ratio(normalize_unicode(question.lower()), normalize_unicode(cached_question.lower())) > 90:
            return cache[key]
    return None

# H√†m ki·ªÉm tra ng·ªØ c·∫£nh li√™n quan
def check_context_relevance(question: str, history: List[Dict]) -> bool:
    if not history:
        return False
    last_entry = history[-1]
    last_question = last_entry.get("sentence", "").lower()
    question_lower = question.lower()
    if any(keyword in question_lower for keyword in DOUBT_KEYWORDS):
        return True
    if fuzz.ratio(normalize_unicode(question_lower), normalize_unicode(last_question)) > 80:
        return True
    return False

# H√†m ph√¢n t√≠ch c√∫ ph√°p c√¢u h·ªèi
def parse_question(question: str) -> Dict[str, str]:
    result = {"main_verb": "", "entities": [], "vehicle_type": None, "time": None, "intent": "unknown"}
    if not nlp_vi:
        return result
    try:
        doc = nlp_vi(normalize_unicode(question))
        result["main_verb"] = next((token.text for token in doc if token.pos_ == "VERB"), "")
        result["entities"] = [ent.text for ent in doc.ents]
        for token in doc:
            if token.text.lower() in ["xe m√°y", "√¥ t√¥", "xe t·∫£i", "xe kh√°ch"]:
                result["vehicle_type"] = token.text
            if token.text.lower() in ["h√¥m nay", "h√¥m qua", "ng√†y mai"] or re.match(r'\d{1,2}/\d{1,2}/\d{4}', token.text):
                result["time"] = token.text
        question_lower = question.lower()
        if any(k in question_lower for k in ["m·ª©c ph·∫°t", "ph·∫°t", "ngh·ªã ƒë·ªãnh"]):
            result["intent"] = "traffic_law"
        elif any(k in question_lower for k in ["bi·ªÉn s·ªë", "vi ph·∫°m"]):
            result["intent"] = "plate_violation"
        elif any(k in question_lower for k in ["giao th√¥ng", "k·∫πt xe", "m·∫≠t ƒë·ªô"]):
            result["intent"] = "traffic_external"
        elif any(k in question_lower for k in ["ch√†o", "hi", "hello"]):
            result["intent"] = "social"
        elif "ph√°t hi·ªán" in question_lower and any(k in question_lower for k in ["ƒë√®n ƒë·ªè", "t·ªëc ƒë·ªô", "m≈© b·∫£o hi·ªÉm"]):
            result["intent"] = "method_violation"
        return result
    except Exception as e:
        logger.error(f"L·ªói ph√¢n t√≠ch c√∫ ph√°p: {e}")
        return result

# H√†m l√†m s·∫°ch c√¢u h·ªèi
def clean_question(sentence: str) -> str:
    if not sentence or not isinstance(sentence, str) or not is_safe_input(sentence):
        return ""
    plate_placeholder = {}
    patterns = [
        r'\b\d{2}[A-Z]{0,1}-\d{3,5}\b',  # V√≠ d·ª•: 51D-2222, 51D-12345
        r'\b\d{2}[A-Z]{0,1}-\d{3}\.\d{2}\b',  # V√≠ d·ª•: 51D-123.45
        r'\b[A-Z]{2}-\d{2}-\d{2,3}\b'  # V√≠ d·ª•: MD-12-123
    ]
    sentence = normalize_unicode(sentence)
    for pattern in patterns:
        match = re.search(pattern, sentence, re.IGNORECASE)
        if match:
            plate = match.group(0)
            placeholder = "__PLATE__"
            plate_placeholder[placeholder] = plate
            sentence = sentence.replace(plate, placeholder)
    sentence = re.sub(r'[^\w\s√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ.,!?]', '', sentence)
    sentence = re.sub(r'\s+', ' ', sentence).strip()
    sentence = auto_correct_spelling(sentence)
    for placeholder, original in plate_placeholder.items():
        sentence = sentence.replace(placeholder, original)
    return sentence

# H√†m s·ª≠a l·ªói ch√≠nh t·∫£ t·ª± ƒë·ªông
def auto_correct_spelling(text: str) -> str:
    if not text:
        return text
    text = normalize_unicode(text)
    text = re.sub(r'(\s*üòä\s*)+', ' üòä', text)
    text = re.sub(r'(\w+)\s+\1', r'\1', text, flags=re.IGNORECASE)
    text = re.sub(r'[^\w\s√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ.,!?]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    corrections = [
        ("ƒë∆∞·ªùng m·ªôt chi·ªÉu", "ƒë∆∞·ªùng m·ªôt chi·ªÅu"), ("giao thong", "giao th√¥ng"), ("giao thongg", "giao th√¥ng"),
        ("k·∫πt x·∫π", "k·∫πt xe"), ("m·ªß b·∫£o hi·ªÉm", "m≈© b·∫£o hi·ªÉm"), ("tai n·∫°m", "tai n·∫°n"), ("t·ªëc ƒë√¥", "t·ªëc ƒë·ªô"),
        ("ƒë√®n ƒëo", "ƒë√®n ƒë·ªè"), ("surtraff error", "b√°o l·ªói h·ªá th·ªëng"), ("ph√°tri·ªÉn", "ph√°t tri·ªÉn"),
        ("bi·∫øth√™m", "bi·∫øt th√™m"), ("c√¥ngh·ªá", "c√¥ng ngh·ªá"), ("tin√†y", "tin n√†y"), ("R·∫•ti·∫øc", "R·∫•t ti·∫øc"),
        ("t√¨nh tr·∫°ngiao", "t√¨nh tr·∫°ng"), ("h·ªèiƒë√°p", "h·ªèi ƒë√°p"), ("tr·∫£l·ªùi", "tr·∫£ l·ªùi"), ("h·ªótr·ª£", "h·ªó tr·ª£"),
        ("ti·∫øpt·ª•c", "ti·∫øp t·ª•c"), ("th√™mth√¥ngtin", "th√™m th√¥ng tin"), ("li√™nquan", "li√™n quan")
    ]
    for wrong, correct in corrections:
        text = re.sub(r'\b' + re.escape(wrong) + r'\b', correct, text, flags=re.IGNORECASE)
    if nlp_vi:
        doc = nlp_vi(text)
        corrected = " ".join(token.text for token in doc if token.text.lower() in valid_vietnamese_words or not token.is_alpha)
        return corrected if corrected.strip() else text
    return text

# H√†m ki·ªÉm tra ch√≠nh t·∫£ ti·∫øng Vi·ªát
def check_vietnamese_spelling(text: str) -> bool:
    if not text or not isinstance(text, str):
        return False
    text = normalize_unicode(text)
    text = re.sub(r'[^\w\s√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ]', '', text)
    words = text.lower().split()
    if not words:
        return False
    return any(word in valid_vietnamese_words for word in words)

# H√†m ghi log tr√≤ chuy·ªán
def log_chat(question: str, response: str, lang: str, topic: str, plate: str = None):
    try:
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "question": normalize_unicode(question),
            "response": normalize_unicode(response),
            "language": lang,
            "topic": topic,
            "license_plate": plate,
            "stack_trace": ""
        }
        with open(CHAT_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        logger.info(f"ƒê√£ ghi log cho c√¢u h·ªèi: {question[:50]}... (plate: {plate})")
    except Exception as e:
        logger.error(f"L·ªói ghi log: {e}")

# H√†m l∆∞u ph·∫£n h·ªìi
def save_feedback(question: str, response: str, lang: str):
    try:
        feedback_entry = {
            "content": normalize_unicode(response),
            "question": normalize_unicode(question),
            "language": lang,
            "timestamp": datetime.now().isoformat(),
            "topic": detect_topic(question)
        }
        with open(FEEDBACK_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(feedback_entry, ensure_ascii=False) + "\n")
        logger.info(f"ƒê√£ l∆∞u ph·∫£n h·ªìi cho c√¢u h·ªèi: {question[:50]}...")
        asyncio.create_task(update_user_index())
    except Exception as e:
        logger.error(f"L·ªói l∆∞u ph·∫£n h·ªìi: {e}")

# H√†m t·∫°o nh√∫ng v·ªõi Gemini API
@retry_decorator(tries=4, delay=1, backoff=2)
def get_gemini_embeddings(texts: List[str], model: str = "text-embedding-004", task_type: str = "SEMANTIC_SIMILARITY", output_dimensionality: int = 512) -> List[np.ndarray]:
    try:
        if isinstance(texts, str):
            texts = [texts]
        texts = [normalize_unicode(text[:1500]) for text in texts if text.strip()]
        if not texts:
            logger.error("Danh s√°ch vƒÉn b·∫£n r·ªóng ho·∫∑c kh√¥ng h·ª£p l·ªá")
            return []
        embeddings = genai.embed_content(
            model=model,
            content=texts,
            task_type=task_type,
            output_dimensionality=output_dimensionality
        )['embedding']
        embeddings = [np.array(e, dtype='float32') for e in embeddings if len(e) == output_dimensionality]
        if not embeddings:
            logger.error("Gemini API tr·∫£ v·ªÅ embeddings r·ªóng")
            return []
        embeddings = [e / np.linalg.norm(e) if np.linalg.norm(e) != 0 else e for e in embeddings]
        logger.info(f"ƒê√£ t·∫°o {len(embeddings)} embeddings, k√≠ch th∆∞·ªõc: {len(embeddings[0]) if embeddings else 0}")
        return embeddings
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o nh√∫ng v·ªõi Gemini API: {str(e)}", exc_info=True)
        raise
# H√†m tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file
def extract_text_from_txt(file_path: str, prioritize_dialogs: bool = False) -> str:
    try:
        if prioritize_dialogs and file_path == KNOWLEDGE_TXT_PATH and os.path.exists(TRAFFIC_DIALOGS_PATH):
            if validate_jsonl_file(TRAFFIC_DIALOGS_PATH):
                dialog_text = []
                with open(TRAFFIC_DIALOGS_PATH, "r", encoding="utf-8-sig") as f:
                    for line in f:
                        if not line.strip():
                            continue
                        try:
                            entry = json.loads(line)
                            question = entry.get("question", "")
                            answers = entry.get("answers", [])
                            if question and answers:
                                dialog_text.append(f"{question} {' '.join(answers)}")
                        except json.JSONDecodeError:
                            logger.warning(f"JSON kh√¥ng h·ª£p l·ªá: {line.strip()}")
                dialog_text = " ".join(set(dialog_text))
                if dialog_text.strip():
                    return normalize_unicode(dialog_text)
        
        with open(file_path, "r", encoding="utf-8-sig") as f:
            text = f.read()
        text = normalize_unicode(text)
        text = auto_correct_spelling(text)
        return text
    except Exception as e:
        logger.error(f"L·ªói tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ {file_path}: {e}")
        return ""

# H√†m tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ URL
@retry_decorator(tries=4, delay=1, backoff=2)
def extract_text_from_url(url: str, max_chars: int = 1500) -> str:
    cache_key = f"web:{url}"
    if cache_key in web_cache:
        logger.info(f"Cache hit cho URL: {url}")
        return web_cache[cache_key]
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        
        for element in soup(["script", "style", "header", "footer", "nav", "aside"]):
            element.decompose()
        
        text_elements = soup.find_all(["p", "h1", "h2", "h3"])
        text = " ".join([elem.get_text(strip=True) for elem in text_elements])
        text = normalize_unicode(text)
        text = re.sub(r'\s+', ' ', text).strip()
        text = text[:max_chars]
        text = auto_correct_spelling(text)
        
        web_cache[cache_key] = text
        logger.info(f"ƒê√£ tr√≠ch xu·∫•t {len(text)} k√Ω t·ª± t·ª´ {url}")
        return text
    except requests.exceptions.RequestException as e:
        logger.error(f"L·ªói tr√≠ch xu·∫•t t·ª´ {url}: {e}")
        return ""

# H√†m ph√°t hi·ªán ch·ªß ƒë·ªÅ
def detect_topic(text: str) -> str:
    if not text.strip():
        return "General"
    text = normalize_unicode(text.lower())
    if any(k in text for k in ["ph·∫°t", "m·ª©c ph·∫°t", "ngh·ªã ƒë·ªãnh", "lu·∫≠t", "b·∫±ng l√°i"]): 
        return "Traffic Law"
    if any(k in text for k in ["c√°c ch·ª©c nƒÉng", "ch·ª©c nƒÉng ph√°t hi·ªán", "detection functions"]): 
        return "SurTraff Functions"
    if any(k in text for k in ["ƒë√®n ƒë·ªè", "red light"]): 
        return "Red Light Detection"
    if any(k in text for k in ["t·ªëc ƒë·ªô", "speed", "overspeed"]): 
        return "Speed Violation"
    if any(k in text for k in ["m≈© b·∫£o hi·ªÉm", "helmet", "no helmet"]): 
        return "Helmet Violation"
    if any(k in text for k in ["tai n·∫°n", "accident"]): 
        return "Accident Detection"
    if any(k in text for k in ["h·ªë", "v·∫≠t c·∫£n", "obstacle", "pothole"]): 
        return "Obstacle Detection"
    if any(k in text for k in ["ƒë·ªó xe", "parking"]): 
        return "Illegal Parking"
    if any(k in text for k in ["sai l√†n", "ng∆∞·ª£c chi·ªÅu", "lane", "wrong-way"]): 
        return "Lane Violation"
    if any(k in text for k in ["m·∫≠t ƒë·ªô", "√πn t·∫Øc", "traffic density"]): 
        return "Traffic Density"
    if any(k in text for k in ["surtraff", "h·ªá th·ªëng", "ƒëƒÉng nh·∫≠p", "login"]): 
        return "SurTraff System"
    if any(k in text for k in ["ph√°t hi·ªán", "nh·∫≠n di·ªán"]) and any(k2 in text for k2 in ["ƒë√®n ƒë·ªè", "t·ªëc ƒë·ªô", "m≈© b·∫£o hi·ªÉm", "bi·ªÉn s·ªë"]): 
        return "Method Violation"
    if any(k in text for k in PLACE_NAMES): 
        return "Traffic Information"
    if any(k in text for k in ["yolo", "camera", "ph√°t hi·ªán", "nh·∫≠n di·ªán", "bi·ªÉn s·ªë", "vi ph·∫°m", "giao th√¥ng th·ªùi gian th·ª±c"]): 
        return "SurTraff Feature"
    return "General"

# Danh s√°ch chi ti·∫øt ch·ª©c nƒÉng SurTraff
def build_surtraff_details():
    details = {
        "Red Light Detection": "SurTraff s·ª≠ d·ª•ng camera AI ƒë·ªìng b·ªô v·ªõi t√≠n hi·ªáu ƒë√®n giao th√¥ng ƒë·ªÉ ph√°t hi·ªán xe v∆∞·ª£t ƒë√®n ƒë·ªè, ch·ª•p 2-3 ·∫£nh v√† video ng·∫Øn, ƒë·∫°t ƒë·ªô ch√≠nh x√°c tr√™n 90%.",
        "Speed Violation": "SurTraff ƒëo t·ªëc ƒë·ªô xe b·∫±ng radar 3D ho·∫∑c camera, so s√°nh v·ªõi gi·ªõi h·∫°n t·ªëc ƒë·ªô c·ªßa ƒëo·∫°n ƒë∆∞·ªùng, t·ª± ƒë·ªông ghi nh·∫≠n vi ph·∫°m n·∫øu v∆∞·ª£t qu√°.",
        "Helmet Violation": "SurTraff s·ª≠ d·ª•ng AI YOLOv8 ƒë·ªÉ ph√°t hi·ªán t√†i x·∫ø ho·∫∑c h√†nh kh√°ch kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm, g·ª≠i c·∫£nh b√°o th·ªùi gian th·ª±c.",
        "Accident Detection": "SurTraff ph√¢n t√≠ch chuy·ªÉn ƒë·ªông v√† ƒë·ªëi t∆∞·ª£ng ƒë·ªÉ ph√°t hi·ªán va ch·∫°m, k√≠ch ho·∫°t c·∫£nh b√°o kh·∫©n c·∫•p v√† h·ªó tr·ª£ ph√¢n t√≠ch video tai n·∫°n.",
        "Illegal Parking": "SurTraff ph√°t hi·ªán xe ƒë·ªó ·ªü khu v·ª±c c·∫•m qu√° 3 ph√∫t, t·ª± ƒë·ªông ghi nh·∫≠n vi ph·∫°m.",
        "Lane Violation": "SurTraff ph√°t hi·ªán xe ƒëi sai l√†n ho·∫∑c ng∆∞·ª£c chi·ªÅu b·∫±ng c√°ch theo d√µi ƒë·ªëi t∆∞·ª£ng, ƒëang th·ª≠ nghi·ªám ·ªü m·ªôt s·ªë khu v·ª±c.",
        "Obstacle Detection": "SurTraff ph√°t hi·ªán h·ªë, c√¢y ƒë·ªï ho·∫∑c v·∫≠t c·∫£n tr√™n ƒë∆∞·ªùng, c·∫≠p nh·∫≠t tr√™n b·∫£n ƒë·ªì h·ªá th·ªëng.",
        "Traffic Density": "SurTraff ph√¢n t√≠ch m·∫≠t ƒë·ªô xe qua camera, cung c·∫•p c·∫£nh b√°o √πn t·∫Øc v√† d·ª± ƒëo√°n t·∫Øc ƒë∆∞·ªùng th·ªùi gian th·ª±c.",
        "SurTraff Functions": "SurTraff h·ªó tr·ª£ ph√°t hi·ªán v∆∞·ª£t ƒë√®n ƒë·ªè, v∆∞·ª£t t·ªëc ƒë·ªô, kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm, ƒë·ªó xe sai, ch·∫°y sai l√†n/ng∆∞·ª£c chi·ªÅu, tai n·∫°n, v·∫≠t c·∫£n, m·∫≠t ƒë·ªô xe, v√† nh·∫≠n di·ªán bi·ªÉn s·ªë.",
        "Traffic Law": """Theo Ngh·ªã ƒë·ªãnh 168/2024/Nƒê-CP (hi·ªáu l·ª±c 1/1/2025):
        - V∆∞·ª£t ƒë√®n ƒë·ªè: Xe m√°y 800.000-1.200.000 ƒë·ªìng, √¥ t√¥ 4-6 tri·ªáu ƒë·ªìng.
        - Kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm: 400.000-600.000 ƒë·ªìng.
        - Ch·∫°y qu√° t·ªëc ƒë·ªô:
          + Xe m√°y: V∆∞·ª£t 5-<10 km/h: 400.000-600.000 ƒë·ªìng; 10-20 km/h: 800.000-1.000.000 ƒë·ªìng; >20 km/h: 6-8 tri·ªáu ƒë·ªìng.
          + √î t√¥: V∆∞·ª£t 5-<10 km/h: 800.000-1.000.000 ƒë·ªìng; 10-20 km/h: 4-6 tri·ªáu ƒë·ªìng; 20-35 km/h: 6-8 tri·ªáu ƒë·ªìng; >35 km/h: 12-14 tri·ªáu ƒë·ªìng.
        - ƒêi ng∆∞·ª£c chi·ªÅu: Xe m√°y 400.000-600.000 ƒë·ªìng, √¥ t√¥ 2-4 tri·ªáu ƒë·ªìng.
        - ƒê·ªó xe sai: Xe m√°y 400.000-600.000 ƒë·ªìng, √¥ t√¥ 800.000-1.200.000 ƒë·ªìng.""",
        "SurTraff Feature": "SurTraff s·ª≠ d·ª•ng YOLOv8 v√† camera AI ƒë·ªÉ nh·∫≠n di·ªán bi·ªÉn s·ªë xe, ph√¢n t√≠ch h√†nh vi l√°i xe nguy hi·ªÉm, qu·∫£n l√Ω ƒë√®n giao th√¥ng, cung c·∫•p b·∫£n ƒë·ªì giao th√¥ng th·ªùi gian th·ª±c, v√† b√°o c√°o vi ph·∫°m theo khu v·ª±c.",
        "SurTraff System": "SurTraff cho ph√©p ƒëƒÉng nh·∫≠p qua t√†i kho·∫£n ƒë∆∞·ª£c c·∫•p b·ªüi h·ªá th·ªëng, s·ª≠ d·ª•ng ·ª©ng d·ª•ng ho·∫∑c website ch√≠nh th·ª©c v·ªõi email v√† m·∫≠t kh·∫©u.",
        "Method Violation": """
        SurTraff ph√°t hi·ªán vi ph·∫°m b·∫±ng camera ANPR v√† AI YOLOv8:
        - **V∆∞·ª£t ƒë√®n ƒë·ªè**: Camera ch·ª•p ·∫£nh/video khi xe v∆∞·ª£t v·∫°ch d·ª´ng l√∫c ƒë√®n ƒë·ªè, ƒë·ªìng b·ªô v·ªõi t√≠n hi·ªáu giao th√¥ng (ƒë·ªô ch√≠nh x√°c >90%).
        - **T·ªëc ƒë·ªô**: ƒêo t·ªëc ƒë·ªô b·∫±ng radar 3D ho·∫∑c camera, so s√°nh v·ªõi gi·ªõi h·∫°n ƒë∆∞·ªùng.
        - **M≈© b·∫£o hi·ªÉm**: AI ph√°t hi·ªán t√†i x·∫ø kh√¥ng ƒë·ªôi m≈© qua video th·ªùi gian th·ª±c.
        D·ªØ li·ªáu vi ph·∫°m ƒë∆∞·ª£c g·ª≠i ƒë·∫øn trung t√¢m ƒëi·ªÅu khi·ªÉn ƒë·ªÉ x·ª≠ l√Ω. üòä"""
    }
    if validate_jsonl_file(TRAFFIC_DIALOGS_PATH):
        with open(TRAFFIC_DIALOGS_PATH, "r", encoding="utf-8-sig") as file:
            for line in file:
                if not line.strip():
                    continue
                try:
                    entry = json.loads(line)
                    question = entry.get("question", "").lower()
                    answers = entry.get("answers", [])
                    topic = detect_topic(question)
                    if topic not in details and answers:
                        details[topic] = " ".join(answers)[:150]
                except json.JSONDecodeError:
                    logger.warning(f"JSON kh√¥ng h·ª£p l·ªá: {line.strip()}")
                    continue
    return details

# H√†m ph√°t hi·ªán ng√¥n ng·ªØ
async def detect_language(text: str, history: List[Dict]) -> str:
    if not text.strip():
        return "vi"
    text_lower = normalize_unicode(text.lower())
    vi_keywords = ["giao th√¥ng", "bi·ªÉn s·ªë", "m≈© b·∫£o hi·ªÉm", "ƒë√®n ƒë·ªè", "t·ªëc ƒë·ªô", "ƒëƒÉng nh·∫≠p", "nh∆∞ th·∫ø n√†o"]
    en_keywords = ["traffic", "license plate", "helmet", "red light", "speed", "login", "how"]
    
    vi_count = sum(1 for k in vi_keywords if k in text_lower)
    en_count = sum(1 for k in en_keywords if k in text_lower)
    
    if vi_count > 0 and en_count == 0:
        return "vi"
    if en_count > 0 and vi_count == 0:
        return "en"
    if vi_count > 0 or en_count > 0:
        return "mixed"
    if history:
        return history[-1].get("lang", "vi")
    
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            f"Detect the language of this text: '{text}'. Return only 'vi' for Vietnamese or 'en' for English.",
            generation_config={"max_output_tokens": 10, "temperature": 0.0}
        )
        lang = response.text.strip()
        return lang if lang in ["vi", "en"] else "vi"
    except Exception as e:
        logger.error(f"L·ªói ph√°t hi·ªán ng√¥n ng·ªØ: {e}")
        return "vi"

# H√†m d·ªãch thu·∫≠t vi->en
async def translate_vi2en(text: str) -> str:
    cache_key = f"vi2en:{normalize_unicode(text)}"
    if cache_key in translation_cache:
        return translation_cache[cache_key]
    try:
        place_map = {}
        text_lower = normalize_unicode(text.lower())
        for vi, en in keyword_map.items():
            if vi in text_lower:
                placeholder = f"__KEYWORD_{len(place_map)}__"
                place_map[placeholder] = en
                text = re.sub(r'\b' + re.escape(vi) + r'\b', placeholder, text, flags=re.IGNORECASE)
                text_lower = text_lower.replace(vi.lower(), placeholder.lower())
        for place in PLACE_NAMES:
            if place.lower() in text_lower:
                placeholder = f"__PLACE_{len(place_map)}__"
                place_map[placeholder] = place
                text = re.sub(r'\b' + re.escape(place) + r'\b', placeholder, text, flags=re.IGNORECASE)
                text_lower = text_lower.replace(place.lower(), placeholder.lower())
        
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            f"Translate the following Vietnamese text to English, preserving traffic-related terms: {text}",
            generation_config={"max_output_tokens": 150, "temperature": 0.7}
        )
        translated = normalize_unicode(response.text.strip())
        translated = auto_correct_spelling(translated)
        for placeholder, original in place_map.items():
            translated = translated.replace(placeholder, original)
        translated = re.sub(r'\s+', ' ', translated).strip()
        translation_cache[cache_key] = translated
        return translated
    except Exception as e:
        logger.error(f"L·ªói d·ªãch vi->en: {e}")
        return text if check_vietnamese_spelling(text) else "Kh√¥ng th·ªÉ d·ªãch c√¢u n√†y, vui l√≤ng th·ª≠ l·∫°i!"

# H√†m d·ªãch thu·∫≠t en->vi
async def translate_en2vi(text: str) -> str:
    cache_key = f"en2vi:{normalize_unicode(text)}"
    if cache_key in translation_cache:
        return translation_cache[cache_key]
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            f"Translate the following English text to Vietnamese, preserving traffic-related terms: {text}",
            generation_config={"max_output_tokens": 150, "temperature": 0.7}
        )
        translated = normalize_unicode(response.text.strip())
        translated = auto_correct_spelling(translated)
        translated = re.sub(r'\s+', ' ', translated).strip()
        if not check_vietnamese_spelling(translated):
            logger.warning(f"D·ªãch en->vi kh√¥ng h·ª£p l·ªá: {translated}")
            return "Kh√¥ng th·ªÉ d·ªãch c√¢u n√†y, vui l√≤ng th·ª≠ l·∫°i!"
        translation_cache[cache_key] = translated
        return translated
    except Exception as e:
        logger.error(f"L·ªói d·ªãch en->vi: {e}")
        return "Kh√¥ng th·ªÉ d·ªãch c√¢u n√†y, vui l√≤ng th·ª≠ l·∫°i!"

# H√†m l·∫•y th·ªùi gian trong ng√†y
def get_time_of_day() -> str:
    current_hour = datetime.now().hour
    if 5 <= current_hour < 12:
        return "morning"
    elif 12 <= current_hour < 17:
        return "afternoon"
    elif 17 <= current_hour < 21:
        return "evening"
    else:
        return "night"

# H√†m ph√°t hi·ªán c·∫£m x√∫c
def detect_emotion(text: str) -> str:
    text_lower = normalize_unicode(text.lower())
    if any(word in text_lower for word in ["g·∫•p", "kh·∫©n c·∫•p", "nguy hi·ªÉm", "urgent", "emergency"]):
        return "urgent"
    if any(word in text_lower for word in ["vui", "happy", "t·ªët", "good", "üòä", "üòÑ"]):
        return "positive"
    if any(word in text_lower for word in ["t·ªá", "x·∫•u", "bad", "terrible", "üòî", "üò¢"]):
        return "negative"
    return "neutral"

# H√†m t√≥m t·∫Øt ng·ªØ c·∫£nh
async def summarize_context(history: List[Dict]) -> str:
    if not history:
        return ""
    history_text = " ".join([entry.get("sentence", "") for entry in history])[:1000]
    if not history_text.strip() or len(history_text.split()) < 5:
        return ""
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(
            f"T√≥m t·∫Øt ng·∫Øn g·ªçn ng·ªØ c·∫£nh t·ª´ c√°c c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát (t·ªëi ƒëa 30 t·ª´): {normalize_unicode(history_text)}",
            generation_config={"max_output_tokens": 30, "temperature": 0.7}
        )
        return normalize_unicode(response.text.strip())
    except Exception as e:
        logger.error(f"L·ªói t√≥m t·∫Øt ng·ªØ c·∫£nh: {e}")
        return ""

# H√†m t·∫°o c√¢u h·ªèi g·ª£i √Ω
async def generate_suggested_questions(history: List[Dict], topic: str, lang: str) -> str:
    try:
        suggestions = {
            "Traffic Law": [
                "M·ª©c ph·∫°t v∆∞·ª£t ƒë√®n ƒë·ªè cho xe m√°y l√† bao nhi√™u?",
                "Ch·∫°y qu√° t·ªëc ƒë·ªô ·ªü th√†nh ph·ªë b·ªã ph·∫°t th·∫ø n√†o?",
                "Lu·∫≠t giao th√¥ng m·ªõi nh·∫•t l√† g√¨?",
                "Ph·∫°t kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm bao nhi√™u ti·ªÅn?"
            ],
            "SurTraff Feature": [
                "SurTraff ph√°t hi·ªán vi ph·∫°m giao th√¥ng nh∆∞ th·∫ø n√†o?",
                "Camera AI c·ªßa SurTraff ho·∫°t ƒë·ªông ra sao?",
                "SurTraff c√≥ h·ªó tr·ª£ b·∫£n ƒë·ªì giao th√¥ng th·ªùi gian th·ª±c kh√¥ng?",
                "SurTraff nh·∫≠n di·ªán bi·ªÉn s·ªë xe th·∫ø n√†o?"
            ],
            "Traffic Information": [
                "T√¨nh tr·∫°ng giao th√¥ng ·ªü H√† N·ªôi h√¥m nay ra sao?",
                "C√≥ k·∫πt xe ·ªü C·∫ßn Th∆° kh√¥ng?",
                "ƒê∆∞·ªùng n√†o ·ªü ƒê√† N·∫µng ƒëang s·ª≠a ch·ªØa?",
                "M·∫≠t ƒë·ªô giao th√¥ng ·ªü H·ªì Ch√≠ Minh th·∫ø n√†o?"
            ],
            "Method Violation": [
                "SurTraff ph√°t hi·ªán vi ph·∫°m t·ªëc ƒë·ªô nh∆∞ th·∫ø n√†o?",
                "L√†m sao SurTraff nh·∫≠n di·ªán kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm?",
                "SurTraff d√πng c√¥ng ngh·ªá g√¨ ƒë·ªÉ ph√°t hi·ªán ch·∫°y ƒë√®n ƒë·ªè?",
                "H·ªá th·ªëng camera c·ªßa SurTraff ho·∫°t ƒë·ªông ra sao?"
            ],
            "General": [
                "SurTraff l√† g√¨?",
                "L√†m sao ƒë·ªÉ ƒëƒÉng nh·∫≠p v√†o SurTraff?",
                "SurTraff h·ªó tr·ª£ nh·ªØng t√≠nh nƒÉng g√¨?",
                "H·ªá th·ªëng SurTraff ho·∫°t ƒë·ªông ·ªü ƒë√¢u?"
            ]
        }
        topic_suggestions = suggestions.get(topic, suggestions["General"])
        if history:
            current_question = history[-1].get("sentence", "").lower()
            topic_suggestions = [s for s in topic_suggestions if fuzz.ratio(normalize_unicode(s.lower()), normalize_unicode(current_question)) < 90]
        suggestion = random.choice(topic_suggestions) if topic_suggestions else "H·ªèi th√™m v·ªÅ giao th√¥ng nh√©!"
        if history:
            suggestion = f"Ti·∫øp n·ªëi c√¢u h·ªèi tr∆∞·ªõc, b·∫°n c√≥ mu·ªën bi·∫øt: {suggestion}"
        else:
            suggestion = f"G·ª£i √Ω: {suggestion}"
        if lang == "en":
            suggestion = await translate_vi2en(suggestion)
        return normalize_unicode(suggestion)
    except Exception as e:
        logger.error(f"L·ªói t·∫°o c√¢u h·ªèi g·ª£i √Ω: {e}")
        return "G·ª£i √Ω: H·ªèi th√™m v·ªÅ giao th√¥ng ho·∫∑c SurTraff nh√©!" if lang == "vi" else "Suggestion: Ask more about traffic or SurTraff!"

# H√†m tr·∫£ l·ªùi x√£ h·ªôi
async def get_social_response(question: str, lang: str, time_of_day: str, history: List[Dict], emotion: str) -> str:
    try:
        greetings = {
            "morning": {
                "positive": "Ch√†o bu·ªïi s√°ng! H√¥m nay b·∫°n vui v·∫ª, mu·ªën bi·∫øt th√™m v·ªÅ giao th√¥ng kh√¥ng? üòä" if lang == "vi" else "Good morning! You're cheerful today, want to know more about traffic? üòä",
                "neutral": "Ch√†o bu·ªïi s√°ng! H√¥m nay b·∫°n mu·ªën h·ªèi g√¨ v·ªÅ giao th√¥ng? üòä" if lang == "vi" else "Good morning! What do you want to ask about traffic today? üòä",
                "negative": "Ch√†o bu·ªïi s√°ng! C√≥ g√¨ kh√¥ng ·ªïn √†? H·ªèi v·ªÅ giao th√¥ng ƒë·ªÉ m√¨nh gi√∫p nh√©! üòä" if lang == "vi" else "Good morning! Something wrong? Ask about traffic, I'll help! üòä",
                "urgent": "Ch√†o bu·ªïi s√°ng! C·∫ßn th√¥ng tin giao th√¥ng g·∫•p √†? H·ªèi ngay n√†o! üö®" if lang == "vi" else "Good morning! Need traffic info urgently? Ask now! üö®"
            },
            "afternoon": {
                "positive": "Ch√†o bu·ªïi chi·ªÅu! T√¢m tr·∫°ng t·ªët nh·ªâ, h·ªèi g√¨ v·ªÅ giao th√¥ng n√†o? üöó" if lang == "vi" else "Good afternoon! Feeling great, what's your traffic question? üöó",
                "neutral": "Ch√†o bu·ªïi chi·ªÅu! C√≥ c·∫ßn th√¥ng tin giao th√¥ng kh√¥ng? üöó" if lang == "vi" else "Good afternoon! Need traffic information? üöó",
                "negative": "Ch√†o bu·ªïi chi·ªÅu! C√≥ g√¨ kh√¥ng ·ªïn? H·ªèi v·ªÅ giao th√¥ng ƒë·ªÉ m√¨nh h·ªó tr·ª£ nh√©! üòä" if lang == "vi" else "Good afternoon! Something wrong? Ask about traffic for help! üòä",
                "urgent": "Ch√†o bu·ªïi chi·ªÅu! C·∫ßn th√¥ng tin giao th√¥ng g·∫•p √†? H·ªèi ngay n√†o! üö®" if lang == "vi" else "Good afternoon! Urgent traffic info needed? Ask now! üö®"
            },
            "evening": {
                "positive": "Ch√†o bu·ªïi t·ªëi! Vui v·∫ª th·∫ø, h·ªèi g√¨ v·ªÅ SurTraff n√†o? üåô" if lang == "vi" else "Good evening! So cheerful, what's up with SurTraff? üåô",
                "neutral": "Ch√†o bu·ªïi t·ªëi! H·ªèi g√¨ v·ªÅ SurTraff n√†o? üåô" if lang == "vi" else "Good evening! What's up with SurTraff? üåô",
                "negative": "Ch√†o bu·ªïi t·ªëi! C√≥ g√¨ kh√¥ng ·ªïn √†? H·ªèi v·ªÅ giao th√¥ng ƒë·ªÉ m√¨nh gi√∫p nh√©! üòä" if lang == "vi" else "Good evening! Something wrong? Ask about traffic for help! üòä",
                "urgent": "Ch√†o bu·ªïi t·ªëi! C·∫ßn th√¥ng tin giao th√¥ng g·∫•p √†? H·ªèi ngay n√†o! üö®" if lang == "vi" else "Good evening! Urgent traffic info needed? Ask now! üö®"
            },
            "night": {
                "positive": "Khuya r·ªìi, v·∫´n vui v·∫ª √†? H·ªèi g√¨ v·ªÅ giao th√¥ng n√†o! üåå" if lang == "vi" else "It's late, still cheerful? Ask about traffic! üåå",
                "neutral": "Khuya r·ªìi, v·∫´n quan t√¢m giao th√¥ng √†? H·ªèi ƒëi! üåå" if lang == "vi" else "It's late! Still curious about traffic? Ask away! üåå",
                "negative": "Khuya r·ªìi, c√≥ g√¨ kh√¥ng ·ªïn √†? H·ªèi v·ªÅ giao th√¥ng ƒë·ªÉ m√¨nh h·ªó tr·ª£ nh√©! üòä" if lang == "vi" else "It's late! Something wrong? Ask about traffic for help! üòä",
                "urgent": "Khuya r·ªìi, c·∫ßn th√¥ng tin giao th√¥ng g·∫•p √†? H·ªèi ngay n√†o! üö®" if lang == "vi" else "It's late! Urgent traffic info needed? Ask now! üö®"
            }
        }
        greeting = greetings.get(time_of_day, {}).get(emotion, "Ch√†o b·∫°n! H·ªèi g√¨ v·ªÅ giao th√¥ng nh√©! üòä" if lang == "vi" else "Hello! Ask about traffic! üòä")
        if history:
            greeting = f"{greeting} Ti·∫øp n·ªëi c√¢u h·ªèi tr∆∞·ªõc, b·∫°n mu·ªën bi·∫øt th√™m g√¨? üòä" if lang == "vi" else f"{greeting} Following your last question, what else do you want to know? üòä"
        return normalize_unicode(greeting)
    except Exception as e:
        logger.error(f"L·ªói tr·∫£ l·ªùi x√£ h·ªôi: {e}")
        return "Ch√†o b·∫°n! H·ªèi g√¨ v·ªÅ giao th√¥ng nh√©! üòä" if lang == "vi" else "Hello! Ask about traffic! üòä"

# H√†m tr√≠ch xu·∫•t bi·ªÉn s·ªë
def extract_plate(text: str) -> Optional[str]:
    patterns = [
        r'\b\d{2}[A-Z]{0,1}-\d{3,5}\b',  # V√≠ d·ª•: 51D-2222, 51D-12345
        r'\b\d{2}[A-Z]{0,1}-\d{3}\.\d{2}\b',  # V√≠ d·ª•: 51D-123.45
        r'\b[A-Z]{2}-\d{2}-\d{2,3}\b'  # V√≠ d·ª•: MD-12-123
    ]
    text = normalize_unicode(text)
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            plate = match.group(0).upper()
            logger.info(f"Nh·∫≠n di·ªán bi·ªÉn s·ªë: {plate}")
            if re.match(r'^\d{2}[A-Z]?-\d{3,5}$', plate) or re.match(r'^\d{2}[A-Z]?-\d{3}\.\d{2}$', plate) or re.match(r'^[A-Z]{2}-\d{2}-\d{2,3}$', plate):
                return plate
    logger.warning(f"Kh√¥ng t√¨m th·∫•y bi·ªÉn s·ªë h·ª£p l·ªá trong: {text}")
    return None

# H√†m ki·ªÉm tra ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi
async def check_answer_quality(question: str, answer: str, lang: str) -> float:
    if len(answer.split()) < 5:
        logger.warning(f"C√¢u tr·∫£ l·ªùi qu√° ng·∫Øn: {answer}")
        return 0.0
    try:
        embeddings = get_gemini_embeddings([normalize_unicode(question), normalize_unicode(answer)], model="text-embedding-004")
        if len(embeddings) != 2:
            logger.error("Kh√¥ng th·ªÉ t·∫°o nh√∫ng cho c√¢u h·ªèi ho·∫∑c c√¢u tr·∫£ l·ªùi")
            return 0.0
        
        question_vec, answer_vec = embeddings
        cosine_sim = np.dot(question_vec, answer_vec) / (np.linalg.norm(question_vec) * np.linalg.norm(answer_vec))
        logger.info(f"Ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi - Cosine similarity: {cosine_sim:.2f}")
        
        if lang == "vi" and not check_vietnamese_spelling(answer):
            logger.warning(f"C√¢u tr·∫£ l·ªùi ti·∫øng Vi·ªát kh√¥ng h·ª£p l·ªá: {answer}")
            return 0.0
        
        return cosine_sim
    except Exception as e:
        logger.error(f"L·ªói ki·ªÉm tra ch·∫•t l∆∞·ª£ng: {e}")
        return 0.0

# H√†m t√¨m ki·∫øm ng·ªØ nghƒ©a
async def semantic_search(query: str, topic: str, k: int = 30) -> List[str]:
    try:
        logger.info(f"Th·ª±c hi·ªán semantic search cho: {query}, type: {topic}")

        # N·∫øu FAISS ch∆∞a c√≥ ‚Üí fallback fuzzy search
        if not vector_official:
            logger.warning("FAISS index ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o, d√πng fuzzy search.")
            return fuzzy_search_surtraff_details(query)

        # N·∫øu topic c√≥ trong surtraff_details
        if topic in surtraff_details:
            return [surtraff_details[topic]]

        # Search trong FAISS
        query_embedding = get_gemini_embeddings([normalize_unicode(query)], task_type="RETRIEVAL_QUERY")[0]
        faiss_results = vector_official.similarity_search_by_vector(query_embedding, k=k, filter={"topic": topic})

        # N·∫øu kh√¥ng c√≥ ‚Üí th·ª≠ General
        if not faiss_results and topic != "General":
            faiss_results = vector_official.similarity_search_by_vector(query_embedding, k=k, filter={"topic": "General"})

        # N·∫øu v·∫´n kh√¥ng ra ‚Üí fuzzy search
        if not faiss_results:
            return fuzzy_search_surtraff_details(query)

        forbidden_terms = ["culture", "tourism", "festival"]
        filtered_docs = [
            normalize_unicode(r.page_content) for r in faiss_results
            if r.page_content.strip() and not any(term in r.page_content.lower() for term in forbidden_terms)
        ]
        return list(dict.fromkeys(filtered_docs))[:10]
    except Exception as e:
        logger.error(f"L·ªói semantic search: {str(e)}")
        return []

def fuzzy_search_surtraff_details(query: str) -> List[str]:
    best_match = None
    best_score = 0
    for key, val in surtraff_details.items():
        score = fuzz.partial_ratio(normalize_unicode(query.lower()), normalize_unicode(key.lower()))
        if score > best_score:
            best_score = score
            best_match = val
    if best_match and best_score >= 70:
        return [best_match]
    return []

# H√†m ƒë·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi
async def format_response(context: str, question: str, history_summary: str, emotion: str, lang: str, parsed_info: Dict[str, str]) -> str:
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        prompt = f"""B·∫°n l√† m·ªôt chatbot c·ªßa Surtraff, SurTraff l√† m·ªôt h·ªá th·ªëng giao th√¥ng th√¥ng minh v·ªõi c√°c t√≠nh nƒÉng nh·∫≠n di·ªán v∆∞·ª£t ƒë√®n ƒë·ªè, v∆∞·ª£t t·ªëc ƒë·ªô, ƒëo m·∫≠t ƒë·ªô xe, nh·∫≠n di·ªán tai n·∫°n, nh·∫≠n di·ªán kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm, nh·∫≠n di·ªán h·ªë ga ƒë·ªông v·∫≠t tr√™n ƒë∆∞·ªùng, ch·∫°y xe sai l√†n, ng∆∞·ª£c chi·ªÅu, ƒë·∫≠u xe sai qui ƒë·ªãnh, b·∫°n h√£y:
        D·ª±a tr√™n ng·ªØ c·∫£nh: {normalize_unicode(context[:800])}
        v√† l·ªãch s·ª≠: {normalize_unicode(history_summary)}
        Th√¥ng tin ph√¢n t√≠ch: ƒê·ªông t·ª´ ch√≠nh: {parsed_info['main_verb']}, Th·ª±c th·ªÉ: {', '.join(parsed_info['entities'])}, Ph∆∞∆°ng ti·ªán: {parsed_info['vehicle_type'] or 'kh√¥ng x√°c ƒë·ªãnh'}, Th·ªùi gian: {parsed_info['time'] or 'kh√¥ng x√°c ƒë·ªãnh'}, √ù ƒë·ªãnh: {parsed_info['intent']}
        Tr·∫£ l·ªùi c√¢u h·ªèi: {normalize_unicode(question)}
        B·∫±ng {'ti·∫øng Vi·ªát' if lang == 'vi' else 'English'}, ng·∫Øn g·ªçn (t·ªëi ƒëa 100 t·ª´), ƒë√∫ng tr·ªçng t√¢m, th√¢n thi·ªán, ph√π h·ª£p c·∫£m x√∫c ({emotion}), c√≥ emoji.
        N·∫øu kh√¥ng c√≥ th√¥ng tin, b·∫°n t·ª± ƒë·ªÅ xu·∫•t c√¢u tr·∫£ l·ªùi n·∫øu c√¢u h·ªèi trong ch·ªß ƒë·ªÅ v·ªÅ giao th√¥ng ho·∫∑c h·ªá th·ªëng surtraff 'Kh√¥ng c√≥ th√¥ng tin chi ti·∫øt, h·ªèi th√™m nh√©! üòä'
        """
        response = model.generate_content(
            prompt,
            generation_config={"max_output_tokens": 100, "temperature": 0.7}
        ).text.strip()
        response = normalize_unicode(response)
        response = auto_correct_spelling(response)
        if lang == "vi" and not check_vietnamese_spelling(response):
            return f"Kh√¥ng c√≥ th√¥ng tin chi ti·∫øt, h·ªèi th√™m nh√©! üòä" if lang == "vi" else f"No detailed information, ask more! üòä"
        return response
    except Exception as e:
        logger.error(f"L·ªói ƒë·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi: {e}")
        return f"Kh√¥ng c√≥ th√¥ng tin chi ti·∫øt, h·ªèi th√™m nh√©! üòä" if lang == "vi" else f"No detailed information, ask more! üòä"

# H√†m ph√¢n lo·∫°i c√¢u h·ªèi
def classify_question_type(question: str, history: List[Dict]) -> str:
    question_lower = normalize_unicode(question.lower())
    parsed_info = parse_question(question_lower)
    if any(keyword in question_lower for keyword in DOUBT_KEYWORDS) and history:
        return history[-1].get("type", "general")
    if "ph√°t hi·ªán" in question_lower and any(kw in question_lower for kw in ["ƒë√®n ƒë·ªè", "t·ªëc ƒë·ªô", "m≈© b·∫£o hi·ªÉm", "vi ph·∫°m", "bi·ªÉn s·ªë"]):
        return "method_violation"
    if parsed_info["intent"] != "unknown":
        return parsed_info["intent"]
    if any(p in question_lower for p in PLACE_NAMES) or any(k in question_lower for k in ["giao th√¥ng", "traffic", "k·∫πt xe", "traffic jam", "m·∫≠t ƒë·ªô", "density", "t√¨nh tr·∫°ng"]):
        return "traffic_external"
    if any(k in question_lower for k in ["hi", "hello", "ch√†o", "how are you", "b·∫°n kh·ªèe kh√¥ng"]):
        return "social"
    if "bi·ªÉn s·ªë" in question_lower or extract_plate(question_lower):
        return "plate_violation"
    return "general"

# H√†m l·∫•y d·ªØ li·ªáu giao th√¥ng t·ª´ web
async def fetch_external_traffic_data(query: str, lang: str, history: List[Dict]) -> str:
    cache_key = f"traffic_external:{normalize_unicode(query)}:{lang}"
    cached_response = check_similar_question(query, semantic_cache["traffic_external"])
    if cached_response:
        logger.info(f"Cache hit cho traffic query: {cache_key}")
        return cached_response
    
    try:
        location = next((place for place in PLACE_NAMES if place.lower() in normalize_unicode(query.lower())), None)
        parsed_info = parse_question(query)
        time = parsed_info["time"] or datetime.now().strftime('%d/%m/%Y')
        if not location:
            response = f"üòä Vui l√≤ng ch·ªâ r√µ ƒë·ªãa ƒëi·ªÉm (nh∆∞ H√† N·ªôi, C·∫ßn Th∆°) ƒë·ªÉ m√¨nh t√¨m th√¥ng tin giao th√¥ng nh√©!" if lang == "vi" else f"üòä Please specify a location (e.g., Hanoi, Can Tho) for traffic information!"
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        if location.lower() == "c·∫ßn th∆°":
            response = f"üòä H√¥m nay ({time}), giao th√¥ng ·ªü C·∫ßn Th∆° c√≥ th·ªÉ b·ªã ·∫£nh h∆∞·ªüng b·ªüi ng·∫≠p l·ª•t tr√™n c√°c tuy·∫øn ƒë∆∞·ªùng nh∆∞ M·∫≠u Th√¢n, Nguy·ªÖn VƒÉn C·ª´ n·∫øu c√≥ m∆∞a l·ªõn. üòä"
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        query_keywords = normalize_unicode(query.lower()).split()
        query_keywords.append(location.lower())
        search_query = " ".join(query_keywords)
        relevant_urls = [url for url in TRAFFIC_NEWS_URLS if any(kw in search_query for kw in [location.lower(), "k·∫πt xe", "traffic"])] or TRAFFIC_NEWS_URLS[:2]
        
        traffic_data = []
        for url in relevant_urls:
            text = extract_text_from_url(url)
            if text and any(k in normalize_unicode(text.lower()) for k in ["giao th√¥ng", "√πn t·∫Øc", "k·∫πt xe", "tai n·∫°n"]):
                traffic_data.append(text)
        
        if not traffic_data:
            response = f"üòî Kh√¥ng t√¨m th·∫•y th√¥ng tin giao th√¥ng cho {location} v√†o ng√†y {time}." if lang == "vi" else f"üòî No traffic information found for {location} on {time}."
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        combined_text = "\n".join(traffic_data)[:3000]
        if not combined_text.strip():
            response = f"üòî Kh√¥ng t√¨m th·∫•y th√¥ng tin giao th√¥ng cho {location} v√†o ng√†y {time}." if lang == "vi" else f"üòî No traffic information found for {location} on {time}."
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        embeddings = get_gemini_embeddings([combined_text, query])
        if len(embeddings) != 2:
            logger.error("Kh√¥ng th·ªÉ t·∫°o nh√∫ng cho d·ªØ li·ªáu giao th√¥ng")
            response = f"üòî Kh√¥ng th·ªÉ x·ª≠ l√Ω th√¥ng tin giao th√¥ng cho {location}." if lang == "vi" else f"üòî Unable to process traffic information for {location}."
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        context_vec, query_vec = embeddings
        cosine_sim = np.dot(context_vec, query_vec) / (np.linalg.norm(context_vec) * np.linalg.norm(query_vec))
        
        if cosine_sim < 0.8:
            response = f"üòî Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p cho {location} v√†o ng√†y {time}." if lang == "vi" else f"üòî No relevant traffic information found for {location} on {time}."
            semantic_cache["traffic_external"][cache_key] = response
            return response
        
        model = genai.GenerativeModel("gemini-1.5-flash")
        prompt = f"T√≥m t·∫Øt th√¥ng tin giao th√¥ng li√™n quan ƒë·∫øn '{normalize_unicode(query)}' t·ª´ d·ªØ li·ªáu sau:\n{normalize_unicode(combined_text)}\nTr·∫£ l·ªùi b·∫±ng {'ti·∫øng Vi·ªát' if lang == 'vi' else 'English'}, ng·∫Øn g·ªçn, t·ªëi ƒëa 100 t·ª´, c√≥ emoji."
        response = model.generate_content(
            prompt,
            generation_config={"max_output_tokens": 100, "temperature": 0.7}
        ).text.strip()
        response = normalize_unicode(response)
        response = auto_correct_spelling(response)
        
        if not check_vietnamese_spelling(response) and lang == "vi":
            response = f"üòä Kh√¥ng c√≥ th√¥ng tin chi ti·∫øt v·ªÅ {location} v√†o ng√†y {time}. H·ªèi th√™m v·ªÅ giao th√¥ng nh√©!" if lang == "vi" else f"üòä No detailed information for {location} on {time}. Ask more about traffic!"
        
        semantic_cache["traffic_external"][cache_key] = response
        return response
    except Exception as e:
        logger.error(f"L·ªói fetch_external_traffic_data: {e}")
        response = f"üòî C√≥ l·ªói khi l·∫•y th√¥ng tin giao th√¥ng cho {location}. Vui l√≤ng th·ª≠ l·∫°i!" if lang == "vi" else f"üòî Error fetching traffic information for {location}. Please try again!"
        semantic_cache["traffic_external"][cache_key] = response
        return response

# H√†m l·∫•y d·ªØ li·ªáu vi ph·∫°m t·ª´ API
@retry_decorator(tries=4, delay=1, backoff=2)
async def fetch_violation_data(plate: str = None, location: str = None, lang: str = "vi") -> str:
    cache_key = f"violation:{plate}:{location}:{lang}"
    cached_response = check_similar_question(f"{plate}:{location}", semantic_cache["plate_violation"])
    if cached_response:
        logger.info(f"Cache hit cho violation query: {cache_key}")
        return cached_response
    
    try:
        api_url = "http://localhost:8081/api/violations"
        headers = {"Content-Type": "application/json"}
        response = requests.get(api_url, headers=headers, timeout=15)
        if response.status_code != 200:
            logger.error(f"API tr·∫£ v·ªÅ m√£ l·ªói: {response.status_code}")
            response_text = f"üòî L·ªói API, kh√¥ng th·ªÉ ki·ªÉm tra vi ph·∫°m. Th·ª≠ l·∫°i sau!" if lang == "vi" else f"üòî API error, cannot check violations. Try again later!"
            semantic_cache["plate_violation"][cache_key] = response_text
            return response_text
        data = response.json()
        
        if not data:
            response_text = f"‚úÖ Kh√¥ng t√¨m th·∫•y vi ph·∫°m cho {'bi·ªÉn s·ªë ' + plate if plate else 'khu v·ª±c ' + location} v√†o ng√†y {datetime.now().strftime('%d/%m/%Y')}." if lang == "vi" else f"‚úÖ No violations found for {'license plate ' + plate if plate else 'location ' + location} on {datetime.now().strftime('%d/%m/%Y')}."
            semantic_cache["plate_violation"][cache_key] = response_text
            return response_text
        
        violations = []
        for item in data:
            vehicle_plate = item.get('vehicle', {}).get('licensePlate')
            item_location = item.get('camera', {}).get('location')
            if not vehicle_plate or not item_location:
                continue
            vehicle_plate = vehicle_plate.upper()
            item_location = item_location.upper()
            if (plate and vehicle_plate != plate.upper()) or (location and item_location != location.upper()):
                continue
            for detail in item.get("violationDetails", []):
                violation_time = datetime.strptime(detail.get('violationTime', ''), '%Y-%m-%dT%H:%M:%S').strftime('%H:%M %d/%m/%Y') if detail.get('violationTime') else "Kh√¥ng x√°c ƒë·ªãnh"
                violation_type = detail.get('violationType', {}).get('typeName', 'Kh√¥ng x√°c ƒë·ªãnh')
                location = detail.get('location', 'Kh√¥ng x√°c ƒë·ªãnh')
                additional_notes = detail.get('additionalNotes', 'Kh√¥ng c√≥ ghi ch√∫')
                status = item.get('status', 'Kh√¥ng x√°c ƒë·ªãnh').lower()
                violations.append(
                    f"- {violation_type.capitalize()} l√∫c {violation_time} t·∫°i {location} ({additional_notes}, tr·∫°ng th√°i: {status})"
                )
        
        if not violations:
            response_text = f"‚úÖ Kh√¥ng t√¨m th·∫•y vi ph·∫°m cho {'bi·ªÉn s·ªë ' + plate if plate else 'khu v·ª±c ' + location} v√†o ng√†y {datetime.now().strftime('%d/%m/%Y')}." if lang == "vi" else f"‚úÖ No violations found for {'license plate ' + plate if plate else 'location ' + location} on {datetime.now().strftime('%d/%m/%Y')}."
        else:
            response_text = f"üö® {'Bi·ªÉn s·ªë ' + plate if plate else 'Khu v·ª±c ' + location} c√≥ vi ph·∫°m:\n" + "\n".join(violations)
        
        response_text = normalize_unicode(response_text)
        semantic_cache["plate_violation"][cache_key] = response_text
        return response_text
    except requests.exceptions.RequestException as e:
        logger.error(f"L·ªói g·ªçi API: {e}")
        response = f"üòî Kh√¥ng th·ªÉ tra c·ª©u vi ph·∫°m cho bi·ªÉn s·ªë {plate}. Vui l√≤ng th·ª≠ l·∫°i sau!" if lang == "vi" else f"üòî Unable to check violations for plate {plate}. Please try again later!"
        semantic_cache["plate_violation"][cache_key] = response
        return response

# H√†m t·∫£i chunks t·ª´ ph·∫£n h·ªìi
def load_feedback_chunks() -> tuple:
    chunks = []
    metadata = []
    try:
        if not os.path.exists(FEEDBACK_FILE):
            logger.info(f"File {FEEDBACK_FILE} kh√¥ng t·ªìn t·∫°i")
            return [], []
        with open(FEEDBACK_FILE, "r", encoding="utf-8-sig") as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    entry = json.loads(line)
                    content = normalize_unicode(entry.get("content", ""))
                    question = normalize_unicode(entry.get("question", ""))
                    topic = entry.get("topic", "General")
                    if content and question:
                        chunk = f"{question} {content}"
                        chunks.append(chunk)
                        metadata.append({"topic": topic, "source": "feedback"})
                except json.JSONDecodeError:
                    logger.warning(f"JSON kh√¥ng h·ª£p l·ªá trong feedback: {line.strip()}")
                    continue
        logger.info(f"ƒê√£ t·∫£i {len(chunks)} feedback chunks")
        return chunks, metadata
    except Exception as e:
        logger.error(f"L·ªói t·∫£i feedback chunks: {str(e)}", exc_info=True)
        return [], []
    
import asyncio
import pickle
import faiss
from typing import List, Dict, Optional
from services.chatbot.surtraff_utils import *
vector_official = None
vector_user = None
surtraff_details = None
text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)
class GeminiEmbeddings(Embeddings):
    def __init__(self, model: str = "text-embedding-004", task_type: str = "SEMANTIC_SIMILARITY", output_dimensionality: int = 512):
        self.model = model
        self.task_type = task_type
        self.output_dimensionality = output_dimensionality

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            embeddings = get_gemini_embeddings(
                texts=texts,
                model=self.model,
                task_type="RETRIEVAL_DOCUMENT",
                output_dimensionality=self.output_dimensionality
            )
            return [e.tolist() for e in embeddings]
        except Exception as e:
            logger.error(f"L·ªói t·∫°o embedding cho documents: {str(e)}", exc_info=True)
            return [[] for _ in texts]

    def embed_query(self, text: str) -> List[float]:
        try:
            embedding = get_gemini_embeddings(
                texts=[text],
                model=self.model,
                task_type="RETRIEVAL_QUERY",
                output_dimensionality=self.output_dimensionality
            )[0]
            return embedding.tolist()
        except Exception as e:
            logger.error(f"L·ªói t·∫°o embedding cho query: {str(e)}", exc_info=True)
            return []
# H√†m c·∫≠p nh·∫≠t FAISS index ng∆∞·ªùi d√πng
async def update_user_index():
    global vector_user
    user_index_path = FAISS_INDEX_PATH + "_user.pkl"
    
    if not check_system_resources() or not check_disk_space(os.path.dirname(FAISS_INDEX_PATH), 1000):
        logger.error("T√†i nguy√™n h·ªá th·ªëng kh√¥ng ƒë·ªß ƒë·ªÉ c·∫≠p nh·∫≠t FAISS index")
        
    
    chunks, metadata = load_feedback_chunks()
    if not chunks:
        logger.info("Kh√¥ng c√≥ d·ªØ li·ªáu ph·∫£n h·ªìi ƒë·ªÉ c·∫≠p nh·∫≠t index")
        return
    
    try:
        embeddings = get_gemini_embeddings(chunks, task_type="RETRIEVAL_DOCUMENT")
        if not embeddings:
            logger.error("Kh√¥ng th·ªÉ t·∫°o embeddings t·ª´ Gemini API")
            return
        
        dimension = len(embeddings[0])
        index = faiss.IndexHNSWFlat(dimension, 32)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 40
        
        # S·ª≠ d·ª•ng GeminiEmbeddings
        vector_user = FAISS.from_texts(
            texts=chunks,
            embedding=GeminiEmbeddings(task_type="RETRIEVAL_DOCUMENT"),
            metadatas=metadata,
            # faiss_index=index
        )
        
        with open(user_index_path, "wb") as f:
            pickle.dump(vector_user, f)
        logger.info(f"ƒê√£ c·∫≠p nh·∫≠t FAISS user index v·ªõi {len(chunks)} chunks (s·ª≠ d·ª•ng CPU)")
    except Exception as e:
        logger.error(f"L·ªói c·∫≠p nh·∫≠t FAISS user index: {str(e)}", exc_info=True)
# H√†m t·∫°o vector FAISS ch√≠nh th·ª©c
# H√†m t·∫°o vector FAISS ch√≠nh th·ª©c
def build_vector_official():
    global vector_official, surtraff_details
    official_index_path = FAISS_INDEX_PATH + "_official.pkl"
    
    if not check_system_resources() or not check_disk_space(os.path.dirname(FAISS_INDEX_PATH), 1000):
        logger.error("T√†i nguy√™n h·ªá th·ªëng kh√¥ng ƒë·ªß ƒë·ªÉ t·∫°o FAISS index")
    
    try:
        if os.path.exists(official_index_path) and os.path.getsize(official_index_path) > 0:
            with open(official_index_path, "rb") as f:
                store = pickle.load(f)
            logger.info("ƒê√£ t·∫£i FAISS official index")
            return store
        else:
            logger.info(f"File {official_index_path} kh√¥ng t·ªìn t·∫°i ho·∫∑c r·ªóng, t·∫°o m·ªõi index")
    except Exception as e:
        logger.error(f"L·ªói t·∫£i FAISS official index: {str(e)}, t·∫°o m·ªõi index")
    
    surtraff_text = extract_text_from_txt(KNOWLEDGE_TXT_PATH, prioritize_dialogs=True)
    social_text = extract_text_from_txt(SOCIAL_TXT_PATH)
    combined_text = f"{surtraff_text}\n{social_text}"
    if not combined_text.strip():
        logger.error("File surtraff_knowledge.txt, traffic_dialogs.txt ho·∫∑c social.txt r·ªóng")
        return None
    
    surtraff_chunks = text_splitter.split_text(combined_text)
    if not surtraff_chunks:
        logger.error("Kh√¥ng c√≥ d·ªØ li·ªáu trong surtraff_chunks")
        return None
    
    surtraff_chunks = list(dict.fromkeys(surtraff_chunks))
    surtraff_metadata = [{"topic": detect_topic(chunk), "source": "surtraff"} for chunk in surtraff_chunks]
    
    try:
        forbidden_terms = ["culture", "tourism", "festival"]
        filtered_chunks = [c for c in surtraff_chunks if c.strip() and not any(term in normalize_unicode(c.lower()) for term in forbidden_terms)]
        if not filtered_chunks:
            logger.error("Kh√¥ng c√≥ chunk h·ª£p l·ªá sau khi l·ªçc")
            return None
        
        embeddings = get_gemini_embeddings(texts=filtered_chunks, task_type="RETRIEVAL_DOCUMENT", output_dimensionality=512)
        if not embeddings:
            logger.error("Kh√¥ng th·ªÉ t·∫°o embeddings t·ª´ Gemini API")
            return None
        
        dimension = len(embeddings[0])
        index = faiss.IndexHNSWFlat(dimension, 32)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 40
        
        store = FAISS.from_texts(
            texts=filtered_chunks,
            embedding=GeminiEmbeddings(task_type="RETRIEVAL_DOCUMENT"),
            metadatas=[m for c, m in zip(surtraff_chunks, surtraff_metadata) if c.strip() and not any(term in normalize_unicode(c.lower()) for term in forbidden_terms)],
            # faiss_index=index
        )
        
        with open(official_index_path, "wb") as f:
            pickle.dump(store, f)
        logger.info(f"ƒê√£ t·∫°o v√† l∆∞u FAISS official index v·ªõi {len(filtered_chunks)} chunks (s·ª≠ d·ª•ng CPU)")
        return store
    except Exception as e:
        logger.error(f"L·ªói t·∫°o FAISS official index: {str(e)}", exc_info=True)
        return None
    
# H√†m t·∫°o vector FAISS cho ng∆∞·ªùi d√πng
def build_vector_user():
    global vector_user
    user_index_path = FAISS_INDEX_PATH + "_user.pkl"
    
    if not check_system_resources() or not check_disk_space(os.path.dirname(FAISS_INDEX_PATH), 1000):
        logger.error("T√†i nguy√™n h·ªá th·ªëng kh√¥ng ƒë·ªß ƒë·ªÉ t·∫°o FAISS user index")
        
    
    try:
        if os.path.exists(user_index_path) and os.path.getsize(user_index_path) > 0:
            with open(user_index_path, "rb") as f:
                store = pickle.load(f)
            logger.info("ƒê√£ t·∫£i FAISS user index")
            return store
        else:
            logger.info(f"File {user_index_path} kh√¥ng t·ªìn t·∫°i ho·∫∑c r·ªóng, t·∫°o m·ªõi index")
    except Exception as e:
        logger.error(f"L·ªói t·∫£i FAISS user index: {str(e)}, t·∫°o m·ªõi index")
    
    chunks, metadata = load_feedback_chunks()
    if not chunks:
        logger.info("Kh√¥ng c√≥ d·ªØ li·ªáu ph·∫£n h·ªìi ƒë·ªÉ t·∫°o index")
        return None
    
    try:
        embeddings = get_gemini_embeddings(chunks, task_type="RETRIEVAL_DOCUMENT")
        if not embeddings:
            logger.error("Kh√¥ng th·ªÉ t·∫°o embeddings t·ª´ Gemini API")
            return None
        
        dimension = len(embeddings[0])
        index = faiss.IndexHNSWFlat(dimension, 32)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 40
        
        vector_user = FAISS.from_texts(
            texts=chunks,
            embedding=GeminiEmbeddings(task_type="RETRIEVAL_DOCUMENT"),
            metadatas=metadata,
            # faiss_index=index
        )
        
        with open(user_index_path, "wb") as f:
            pickle.dump(vector_user, f)
        logger.info(f"ƒê√£ t·∫°o v√† l∆∞u FAISS user index v·ªõi {len(chunks)} chunks (s·ª≠ d·ª•ng CPU)")
        return vector_user
    except Exception as e:
        logger.error(f"L·ªói t·∫°o FAISS user index: {str(e)}", exc_info=True)
        return None

# Kh·ªüi t·∫°o surtraff_details
surtraff_details = build_surtraff_details()
if not surtraff_details:
    logger.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o surtraff_details, s·ª≠ d·ª•ng dictionary r·ªóng")
    surtraff_details = {}

# Kh·ªüi t·∫°o FAISS index
vector_official = build_vector_official()
if not vector_official:
    logger.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o FAISS official index")

vector_user = build_vector_user()
if not vector_user:
    logger.info("Kh√¥ng th·ªÉ kh·ªüi t·∫°o FAISS user index, s·∫Ω t·∫°o khi c√≥ ph·∫£n h·ªìi")
