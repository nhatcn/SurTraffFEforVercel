import os
import cv2
import math
import time
from datetime import datetime
from collections import deque

from ultralytics import YOLO
from database import SessionLocal
from models.model import Violation
from schemas.violation_schema import ViolationCreate
from utils.yt_stream import get_stream_url
from crud import violation_crud  
import traceback
import numpy as np


model_vehicle = YOLO("yolov8m.pt")
model_light = YOLO("final.pt")  # ƒê√®n ƒë·ªè

stop_line_y = 550
iou_threshold = 200
red_light_buffer_size = 3

# Create violations directory if it doesn't exist
VIOLATIONS_DIR = "violations"
os.makedirs(VIOLATIONS_DIR, exist_ok=True)

def stream_normal_video_service(youtube_url: str):
    stream_url = get_stream_url(youtube_url)
    cap = cv2.VideoCapture(stream_url)
    if not cap.isOpened():
        raise ValueError("Cannot open stream")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                cap.release()
                cap = cv2.VideoCapture(stream_url)
                continue

            results = model_vehicle(frame)
            annotated = results[0].plot()

            _, jpeg = cv2.imencode('.jpg', annotated)
            yield (
                b"--frame\r\n"
                b"Content-Type: image/jpeg\r\n\r\n" + jpeg.tobytes() + b"\r\n"
            )
    finally:
        cap.release()
        
def stream_violation_video_service(youtube_url: str, camera_id: int):
    stream_url = get_stream_url(youtube_url)
    cap = cv2.VideoCapture(stream_url)
    if not cap.isOpened():
        raise ValueError("Cannot open stream")

    red_light_history = []
    vehicle_states = {}  # track_id: {prev_y, violated, video_saved}
    frame_buffer = deque(maxlen=30)  # 1 gi√¢y (30fps)
    recording_tasks = {}  # track_id: {writer, frames_remaining, file_path}
    db = SessionLocal()

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                cap.release()
                cap = cv2.VideoCapture(stream_url)
                continue

            h, w, _ = frame.shape
            stop_line_y = h * 4 // 10

            # 1 b·∫£n ƒë·ªÉ annotate ƒë·∫ßy ƒë·ªß hi·ªÉn th·ªã stream
            frame_annotated = frame.copy()

            # 1 b·∫£n ri√™ng ch·ªâ v·∫Ω line v√† xe vi ph·∫°m (ƒë·ªÉ ghi video)
            frame_for_video = frame.copy()

            # Detect ƒë√®n ƒë·ªè
            light_results = model_light(frame)[0]
            red_detected = False
            for box in light_results.boxes:
                cls_id = int(box.cls[0])
                conf = float(box.conf[0])
                if model_light.names[cls_id].lower() == 'red' and conf > 0.5:
                    red_detected = True
                    break

            red_light_history.append(red_detected)
            if len(red_light_history) > red_light_buffer_size:
                red_light_history.pop(0)

            is_red = red_light_history.count(True) > red_light_buffer_size // 2

            # V·∫Ω v·∫°ch d·ª´ng v√† tr·∫°ng th√°i ƒë√®n l√™n c·∫£ 2 b·∫£n
            line_color = (0, 0, 255) if is_red else (0, 255, 0)
            for f in [frame_annotated, frame_for_video]:
                cv2.line(f, (0, stop_line_y), (w, stop_line_y), line_color, 1)
                cv2.putText(f, f"Red Light: {'YES' if is_red else 'NO'}", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, line_color, 2)

            results = model_vehicle.track(source=frame, persist=True, conf=0.3, iou=0.5, tracker="bytetrack.yaml")[0]
            if results.boxes is None or results.boxes.id is None:
                frame_buffer.append(frame_for_video.copy())
                continue

            boxes = results.boxes
            for i in range(len(boxes)):
                cls_id = int(boxes.cls[i])
                class_name = model_vehicle.names[cls_id]
                conf = float(boxes.conf[i])
                track_id = int(boxes.id[i])

                if class_name not in ['car', 'truck', 'bus', 'motorbike', 'bicycle'] or conf < 0.3:
                    continue

                x1, y1, x2, y2 = map(int, boxes.xyxy[i])
                center_y = (y1 + y2) // 2

                if track_id not in vehicle_states:
                    vehicle_states[track_id] = {'prev_y': center_y, 'violated': False, 'video_saved': False}
                else:
                    prev_y = vehicle_states[track_id]['prev_y']
                    violated = vehicle_states[track_id]['violated']
                    video_saved = vehicle_states[track_id]['video_saved']

                    if is_red and not violated:
                        if prev_y > stop_line_y >= center_y:
                            vehicle_states[track_id]['violated'] = True

                            if not video_saved:
                                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                                filename = f"violation_{track_id}_{timestamp}.mp4"
                                filepath = os.path.join(VIOLATIONS_DIR, filename)

                                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                                fps = 30
                                writer = cv2.VideoWriter(filepath, fourcc, fps, (w, h))

                                for buf_frame in frame_buffer:
                                    writer.write(buf_frame)

                                recording_tasks[track_id] = {
                                    'writer': writer,
                                    'frames_remaining': 30,
                                    'file_path': filepath
                                }

                                vehicle_states[track_id]['video_saved'] = True

                    vehicle_states[track_id]['prev_y'] = center_y

                violated = vehicle_states[track_id]['violated']

                if violated:
                    # V·∫Ω box ch·ªâ cho xe vi ph·∫°m v√†o c·∫£ stream v√† video
                    color = (0, 0, 255)
                    label = f"ID:{track_id}|violation"
                    for f in [frame_annotated, frame_for_video]:
                        cv2.rectangle(f, (x1, y1), (x2, y2), color, 1)
                        cv2.putText(f, label, (x1, y1 - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 1)
                else:
                    # V·∫Ω l√™n frame stream th√¥i (kh√¥ng buffer)
                    cv2.rectangle(frame_annotated, (x1, y1), (x2, y2), (0, 255, 0), 1)
                    cv2.putText(frame_annotated, f"ID:{track_id}|normal", (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 1)

            # Buffer b·∫£n ghi video (ch·ªâ line + xe vi ph·∫°m)
            frame_buffer.append(frame_for_video.copy())

            # X·ª≠ l√Ω c√°c video ƒëang ghi
            for track_id in list(recording_tasks.keys()):
                task = recording_tasks[track_id]
                if task['frames_remaining'] > 0:
                    task['writer'].write(frame_for_video)
                    task['frames_remaining'] -= 1
                else:
                    task['writer'].release()
                    filepath = task['file_path']

                    violation = ViolationCreate(
                        camera_id=camera_id,
                        violation_type_id=1,
                        license_plate="Unknown",
                        vehicle_color="Unknown",
                        vehicle_brand="Unknown",
                        image_url=filepath,
                        violation_time=datetime.now()
                    )
                    try:
                        db_violation = Violation(**violation.model_dump())
                        db.add(db_violation)
                        db.commit()
                    except Exception as e:
                        print(f"Error saving violation to database: {e}")
                        db.rollback()

                    del recording_tasks[track_id]

            _, jpeg = cv2.imencode('.jpg', frame_annotated)
            yield (
                b"--frame\r\n"
                b"Content-Type: image/jpeg\r\n\r\n" + jpeg.tobytes() + b"\r\n"
            )

    finally:
        cap.release()
        db.close()
        for task in recording_tasks.values():
            task['writer'].release()


def stream_count_video_service(youtube_url: str):
    stream_url = get_stream_url(youtube_url)
    cap = cv2.VideoCapture(stream_url)
    if not cap.isOpened():
        raise ValueError("Cannot open stream")

    model = YOLO("yolov8m.pt")
    vehicle_ids = {}
    in_count = {}
    out_count = {}

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                cap.release()
                cap = cv2.VideoCapture(stream_url)
                continue

            h, w, _ = frame.shape
            line_y = int(h * 0.8)

            results = model.track(source=frame, persist=True, conf=0.3, iou=0.5, tracker="bytetrack.yaml")[0]

            if results.boxes is None or results.boxes.id is None:
                continue

            boxes = results.boxes
            for i in range(len(boxes)):
                cls_id = int(boxes.cls[i])
                class_name = model.names[cls_id]
                conf = float(boxes.conf[i])
                track_id = int(boxes.id[i])

                if class_name not in ['car', 'truck', 'bus', 'motorbike', 'bicycle'] or conf < 0.3:
                    continue

                x1, y1, x2, y2 = map(int, boxes.xyxy[i])
                center_y = (y1 + y2) // 2

                if track_id not in vehicle_ids:
                    vehicle_ids[track_id] = {'prev_y': center_y, 'counted': False, 'class_name': class_name}
                else:
                    prev_y = vehicle_ids[track_id]['prev_y']

                    if not vehicle_ids[track_id]['counted']:
                        if prev_y < line_y <= center_y:
                            in_count[class_name] = in_count.get(class_name, 0) + 1
                            vehicle_ids[track_id]['counted'] = True
                        elif prev_y > line_y >= center_y:
                            out_count[class_name] = out_count.get(class_name, 0) + 1
                            vehicle_ids[track_id]['counted'] = True

                    vehicle_ids[track_id]['prev_y'] = center_y

                # Draw box and ID
                color = (255, 255, 255)
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 1)
                cv2.putText(frame, f"ID:{track_id}, {class_name}", (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)

            # Draw count line
            cv2.line(frame, (0, line_y), (w, line_y), (0, 255, 255), 1)

            # Display per-class IN and OUT counts
            y_offset = 40
            for class_name in sorted(set(in_count.keys()) | set(out_count.keys())):
                in_c = in_count.get(class_name, 0)
                out_c = out_count.get(class_name, 0)
                cv2.putText(frame, f"{class_name.upper()} IN: {in_c}", (10, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
                y_offset += 30
                cv2.putText(frame, f"{class_name.upper()} OUT: {out_c}", (10, y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)
                y_offset += 30

            _, jpeg = cv2.imencode('.jpg', frame)
            yield (
                b"--frame\r\n"
                b"Content-Type: image/jpeg\r\n\r\n" + jpeg.tobytes() + b"\r\n"
            )
    finally:
        cap.release()

def stream_accident_video_service(youtube_url: str):
    stream_url = get_stream_url(youtube_url)
    cap = cv2.VideoCapture(stream_url)
    if not cap.isOpened():
        raise ValueError("Cannot open stream")

    model_accident = YOLO("accident.pt")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                cap.release()
                cap = cv2.VideoCapture(stream_url)
                continue

            results = model_accident(frame)[0]
            annotated_frame = frame.copy()

            for box in results.boxes:
                cls_id = int(box.cls[0])
                conf = float(box.conf[0])
                class_name = model_accident.names[cls_id]

                if conf < 0.5:
                    continue

                x1, y1, x2, y2 = map(int, box.xyxy[0])
                color = (0, 0, 255) if "accident" in class_name.lower() else (0, 255, 0)
                label = f"{class_name} {conf:.2f}"

                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(annotated_frame, label, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

            _, jpeg = cv2.imencode('.jpg', annotated_frame)
            yield (
                b"--frame\r\n"
                b"Content-Type: image/jpeg\r\n\r\n" + jpeg.tobytes() + b"\r\n"
            )
    finally:
        cap.release()

def stream_plate_with_ocr_video_service(youtube_url: str):
    from ultralytics import YOLO
    import cv2

    model_plate = YOLO("best90.pt")  # Model ph√°t hi·ªán bi·ªÉn s·ªë
    stream_url = get_stream_url(youtube_url)
    cap = cv2.VideoCapture(stream_url)

    if not cap.isOpened():
        raise ValueError("Cannot open stream")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                cap.release()
                cap = cv2.VideoCapture(stream_url)
                continue

            results = model_plate(frame)[0]

            for box in results.boxes:
                cls_id = int(box.cls[0])
                class_name = model_plate.names[cls_id]
                conf = float(box.conf[0])

                # T·ªça ƒë·ªô bounding box
                x1, y1, x2, y2 = map(int, box.xyxy[0])

                # V·∫Ω box
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

                # Ghi nh√£n class
                label = f"{class_name}"  # ho·∫∑c th√™m f"{class_name} {conf:.2f}"
                cv2.putText(frame, label, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

            # Encode v√† yield MJPEG stream
            _, jpeg = cv2.imencode('.jpg', frame)
            yield (
                b"--frame\r\n"
                b"Content-Type: image/jpeg\r\n\r\n" + jpeg.tobytes() + b"\r\n"
            )

    finally:
        cap.release()
        
def put_text_with_background(img, text, org, font_scale=1, thickness=1, text_color=(255, 255, 255), bg_color=(0, 0, 0)):
    import cv2
    font = cv2.FONT_HERSHEY_SIMPLEX
    (w, h), _ = cv2.getTextSize(text, font, font_scale, thickness)
    x, y = org
    # V·∫Ω h√¨nh ch·ªØ nh·∫≠t l√†m n·ªÅn
    cv2.rectangle(img, (x, y - h - 5), (x + w, y + 5), bg_color, -1)
    # V·∫Ω text l√™n tr√™n
    cv2.putText(img, text, (x, y), font, font_scale, text_color, thickness, cv2.LINE_AA)

def stream_violation_wrongway_video_service(youtube_url: str, camera_id: int, db=None):
    from ultralytics import YOLO
    import cv2
    import numpy as np
    import time
    import uuid
    from collections import defaultdict

    # --- Load Models ---
    model_sign_path = "trafficsign.pt"
    model_vehicle_path = "yolov8m.pt"

    try:
        model_sign = YOLO(model_sign_path)
        model_vehicle = YOLO(model_vehicle_path)
    except Exception as e:
        print(f"‚ùå Error loading models: {e}")
        return

    # --- Get stream URL from your function (you keep your original) ---
    stream_url = get_stream_url(youtube_url)
    cap = cv2.VideoCapture(stream_url)

    if not cap.isOpened():
        raise ValueError(f"‚ùå Cannot open stream from {stream_url}")

    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    width, height = 640, 640
    scale_x, scale_y = width / original_width, height / original_height

    original_zones_data = [
        ([(744, 404), (212, 1016), (540, 1044), (832, 428)], (0, 255, 0)),
        ([(828, 424), (560, 1016), (932, 1028), (936, 428)], (0, 0, 255)),
        ([(956, 440), (1016, 1024), (1372, 1004), (1060, 444)], (0, 0, 255)),
        ([(1064, 448), (1376, 1024), (1688, 984), (1164, 444)], (0, 255, 0)),
    ]
    zone_allowed_vehicles = {
        0: ['motorcycle'], 1: ['car', 'truck'], 2: ['car', 'truck'], 3: ['motorcycle']
    }

    scaled_zones = []
    for zone_points, color in original_zones_data:
        scaled_zones.append((
            [(int(x * scale_x), int(y * scale_y)) for x, y in zone_points], color
        ))

    LINE_Y = int(height * 0.5)
    DIRECTION_THRESHOLD = 5
    object_tracks = defaultdict(list)
    OBJECT_SIZE, MARGIN, TEXT_HEIGHT = 64, 10, 25
    target_vehicle_classes = ['car', 'motorcycle', 'truck']

    print(f"üî¥ Starting camera {camera_id}, resolution: {original_width}x{original_height}")

    # M√¨nh s·∫Ω d√πng 1 bi·∫øn global id_counter ƒë·ªÉ t·∫°o ID t·∫°m tracking
    id_counter = 0

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("‚ö†Ô∏è Reconnecting...")
                cap.release()
                time.sleep(1)
                cap = cv2.VideoCapture(stream_url)
                if not cap.isOpened():
                    break
                continue

            resized_frame = cv2.resize(frame, (width, height))
            annotated_frame = resized_frame.copy()

            alpha = 0.4
            for zone_points, color in scaled_zones:
                overlay = annotated_frame.copy()
                pts = np.array(zone_points, np.int32).reshape((-1, 1, 2))
                cv2.fillPoly(overlay, [pts], color)
                cv2.addWeighted(overlay, alpha, annotated_frame, 1 - alpha, 0, annotated_frame)
                cv2.polylines(annotated_frame, [pts], True, color, 2)

            results_sign = model_sign(resized_frame, conf=0.1)
            boxes_sign = results_sign[0].boxes
            display_y = MARGIN

            for box in boxes_sign:
                cls_id = int(box.cls)
                label = model_sign.names[cls_id]
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                x1, y1, x2, y2 = max(0, x1), max(0, y1), min(width, x2), min(height, y2)
                if x2 <= x1 or y2 <= y1:
                    continue

                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 165, 255), 2)
                crop = resized_frame[y1:y2, x1:x2]
                if crop.size == 0:
                    continue
                thumb = cv2.resize(crop, (OBJECT_SIZE, OBJECT_SIZE))

                if display_y + OBJECT_SIZE + MARGIN + TEXT_HEIGHT > height:
                    break

                x_thumb = width - OBJECT_SIZE - MARGIN
                annotated_frame[display_y:display_y+OBJECT_SIZE, x_thumb:x_thumb+OBJECT_SIZE] = thumb
                put_text_with_background(annotated_frame, label, (x_thumb, display_y+OBJECT_SIZE+MARGIN), font_scale=0.6)

                display_y += OBJECT_SIZE + MARGIN + TEXT_HEIGHT + MARGIN

            results_vehicle = model_vehicle(resized_frame, conf=0.3)
            # T·∫°o 1 list t·∫°m ƒë·ªÉ l∆∞u ID c·ªßa c√°c object hi·ªán frame n√†y
            current_ids = []

            for box in results_vehicle[0].boxes:
                cls_id = int(box.cls[0])
                cls_name = model_vehicle.names[cls_id]

                if cls_name not in target_vehicle_classes:
                    continue

                x1, y1, x2, y2 = map(int, box.xyxy[0])
                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
                label = f"{cls_name} {box.conf[0]:.2f}"
                bbox_color = (255, 255, 255)
                is_violation = False

                # T·∫°o ID object b·∫±ng c√°ch tƒÉng d·∫ßn (ƒë∆°n gi·∫£n)
                id_counter += 1
                object_id = f"id{id_counter}"
                current_ids.append(object_id)
                object_tracks[object_id].append((cx, cy))
                if len(object_tracks[object_id]) > DIRECTION_THRESHOLD:
                    object_tracks[object_id] = object_tracks[object_id][-DIRECTION_THRESHOLD:]

                if len(object_tracks[object_id]) >= 2:
                    dy = object_tracks[object_id][-1][1] - object_tracks[object_id][0][1]
                    if dy < -15:  # moving up
                        label += " - WRONG WAY"
                        bbox_color = (0, 0, 255)
                        is_violation = True

                for zone_idx, (zone_points, _) in enumerate(scaled_zones):
                    pts = np.array(zone_points, np.int32).reshape((-1, 2))
                    if cv2.pointPolygonTest(pts, (cx, cy), False) >= 0:
                        if cls_name not in zone_allowed_vehicles.get(zone_idx, []):
                            label += " - ZONE VIOLATION"
                            bbox_color = (0, 0, 255)
                            is_violation = True
                        break

                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), bbox_color, 2)
                put_text_with_background(annotated_frame, label, (x1, y1 - 5), font_scale=0.5)
                cv2.circle(annotated_frame, (cx, cy), 4, (0, 255, 255), -1)

            # X√≥a track c·ªßa object kh√¥ng c√≤n xu·∫•t hi·ªán trong frame
            for obj_id in list(object_tracks.keys()):
                if obj_id not in current_ids:
                    del object_tracks[obj_id]

            # cv2.line(annotated_frame, (0, LINE_Y), (width, LINE_Y), (0, 255, 0), 2)
            # put_text_with_background(annotated_frame, "Correct Direction ‚Üí ‚Üì", (10, LINE_Y - 10), font_scale=0.6)

            _, jpeg = cv2.imencode('.jpg', annotated_frame)
            yield (
                b"--frame\r\n"
                b"Content-Type: image/jpeg\r\n\r\n" + jpeg.tobytes() + b"\r\n"
            )

    finally:
        if cap.isOpened():
            cap.release()
        print("‚úÖ Stream ended.")

        
def stream_violation_video_service1(youtube_url: str, camera_id: int):
    import requests
    import json
    import numpy as np
    import cv2
    from shapely.geometry import Point, Polygon

    # K√çCH TH∆Ø·ªöC CHU·∫®N - ph·∫£i gi·ªëng v·ªõi frontend
    STANDARD_WIDTH = 640
    STANDARD_HEIGHT = 480

    # Load models
    model_vehicle = YOLO("yolov8m.pt")
    model_light = YOLO("final.pt")

    # Load zones from Spring Boot API
    def fetch_camera_config(cid: int):
        url = f"http://localhost:8081/api/cameras/{cid}"
        try:
            res = requests.get(url)
            res.raise_for_status()
            return res.json()
        except Exception as e:
            print(f"Error fetching zones: {e}")
            return None

    camera_config = fetch_camera_config(camera_id)
    if not camera_config:
        raise ValueError("Could not fetch camera config")

    zones = camera_config["zones"]
    lane_movements = camera_config["laneMovements"]
    light_lane_links = camera_config["zoneLightLaneLinks"]

    def convert_coordinates_to_frame_size(standard_coords, frame_width, frame_height):
        """
        Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô t·ª´ k√≠ch th∆∞·ªõc chu·∫©n sang k√≠ch th∆∞·ªõc frame th·ª±c t·∫ø
        """
        scale_x = frame_width / STANDARD_WIDTH
        scale_y = frame_height / STANDARD_HEIGHT
        
        converted_coords = []
        for x, y in standard_coords:
            new_x = int(round(x * scale_x))
            new_y = int(round(y * scale_y))
            converted_coords.append([new_x, new_y])
        
        return np.array(converted_coords, dtype=np.int32)

    def point_below_line(point, line_start, line_end):
        """
        Ki·ªÉm tra ƒëi·ªÉm c√≥ ·ªü ph√≠a d∆∞·ªõi ƒë∆∞·ªùng th·∫≥ng kh√¥ng (t·ª´ d∆∞·ªõi m√†n h√¨nh l√™n tr√™n)
        Tr·∫£ v·ªÅ True n·∫øu ƒëi·ªÉm ·ªü ph√≠a d∆∞·ªõi ƒë∆∞·ªùng
        """
        x, y = point
        x1, y1 = line_start
        x2, y2 = line_end
        
        # T√≠nh t√≠ch c√≥ h∆∞·ªõng ƒë·ªÉ x√°c ƒë·ªãnh v·ªã tr√≠ ƒëi·ªÉm so v·ªõi ƒë∆∞·ªùng th·∫≥ng
        cross_product = (x2 - x1) * (y - y1) - (y2 - y1) * (x - x1)
        
        # N·∫øu cross_product > 0, ƒëi·ªÉm ·ªü b√™n tr√°i ƒë∆∞·ªùng (ph√≠a d∆∞·ªõi trong h·ªá t·ªça ƒë·ªô m√†n h√¨nh)
        # N·∫øu cross_product < 0, ƒëi·ªÉm ·ªü b√™n ph·∫£i ƒë∆∞·ªùng (ph√≠a tr√™n trong h·ªá t·ªça ƒë·ªô m√†n h√¨nh) 
        return cross_product > 0

    def detect_line_crossing(prev_point, curr_point, line_start, line_end):
        """
        Ki·ªÉm tra xe c√≥ v∆∞·ª£t qua line t·ª´ d∆∞·ªõi l√™n tr√™n kh√¥ng
        """
        if prev_point is None or curr_point is None:
            return False
            
        # Ki·ªÉm tra xe di chuy·ªÉn t·ª´ ph√≠a d∆∞·ªõi line l√™n ph√≠a tr√™n line
        was_below = point_below_line(prev_point, line_start, line_end)
        is_above = not point_below_line(curr_point, line_start, line_end)
        
        return was_below and is_above

    # Parse zones v√† chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán sau khi bi·∫øt k√≠ch th∆∞·ªõc frame
    lane_zones_standard = {}
    light_zones_standard = {}
    zone_lines_standard = []

    for z in zones:
        # T·ªça ƒë·ªô t·ª´ DB l√† t·ªça ƒë·ªô chu·∫©n (640x480)
        standard_points = json.loads(z["coordinates"])
        
        if z["zoneType"] == "lane":
            lane_zones_standard[z["id"]] = {
                "name": z["name"], 
                "coordinates": standard_points
            }
        elif z["zoneType"] == "light":
            light_zones_standard[z["id"]] = {
                "name": z["name"], 
                "coordinates": standard_points
            }
        elif z["zoneType"] == "line":
            zone_lines_standard.append({
                "id": z["id"],
                "name": z["name"],
                "coordinates": standard_points
            })

    # Build movement and light mappings
    lane_transitions = {(m["fromLaneZoneId"], m["toLaneZoneId"]) for m in lane_movements}
    light_control_map = {link["laneZoneId"]: link["lightZoneId"] for link in light_lane_links}

    stream_url = get_stream_url(youtube_url)
    cap = cv2.VideoCapture(stream_url)

    if not cap.isOpened():
        raise ValueError("Cannot open stream")

    # Bi·∫øn ƒë·ªÉ l∆∞u zones ƒë√£ chuy·ªÉn ƒë·ªïi
    lane_zones = {}
    light_zones = {}
    zone_lines = []
    frame_size_initialized = False

    red_light_history = []
    track_zone_history = {}  # track_id: current_zone_id
    track_position_history = {}  # track_id: previous_position
    vehicle_violations = {}
    vehicle_violation_types = {}  # track_id: violation_type

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                cap.release()
                cap = cv2.VideoCapture(stream_url)
                continue

            frame_annotated = frame.copy()
            h, w, _ = frame.shape

            # Kh·ªüi t·∫°o zones v·ªõi k√≠ch th∆∞·ªõc frame th·ª±c t·∫ø (ch·ªâ l√†m 1 l·∫ßn)
            if not frame_size_initialized:
                print(f"Frame size: {w}x{h}, Standard size: {STANDARD_WIDTH}x{STANDARD_HEIGHT}")
                
                # Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô t·ª´ chu·∫©n sang frame th·ª±c t·∫ø
                for zone_id, zone_data in lane_zones_standard.items():
                    frame_coords = convert_coordinates_to_frame_size(
                        zone_data["coordinates"], w, h
                    )
                    lane_zones[zone_id] = {
                        "name": zone_data["name"],
                        "polygon": frame_coords
                    }

                for zone_id, zone_data in light_zones_standard.items():
                    frame_coords = convert_coordinates_to_frame_size(
                        zone_data["coordinates"], w, h
                    )
                    light_zones[zone_id] = {
                        "name": zone_data["name"],
                        "polygon": frame_coords
                    }

                for line_data in zone_lines_standard:
                    frame_coords = convert_coordinates_to_frame_size(line_data["coordinates"], w, h)
                    zone_lines.append({
                        "id": line_data["id"],
                        "name": line_data["name"],
                        "coordinates": frame_coords
                    })

                frame_size_initialized = True
                print(f"Initialized {len(lane_zones)} lane zones, {len(light_zones)} light zones, {len(zone_lines)} lines")

            # Step 1: V·∫Ω zones l√™n frame
            for zone in lane_zones.values():
                cv2.polylines(frame_annotated, [zone["polygon"]], isClosed=True, color=(255, 255, 0), thickness=2)
                cx, cy = np.mean(zone["polygon"], axis=0).astype(int)
                cv2.putText(frame_annotated, zone["name"], (cx, cy), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)

            for zone in light_zones.values():
                cv2.polylines(frame_annotated, [zone["polygon"]], isClosed=True, color=(0, 0, 255), thickness=2)
                cx, cy = np.mean(zone["polygon"], axis=0).astype(int)
                cv2.putText(frame_annotated, zone["name"], (cx, cy), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

            for line in zone_lines:
                cv2.polylines(frame_annotated, [line["coordinates"]], isClosed=False, color=(0, 255, 255), thickness=3)

            # Step 2: Detect ƒë√®n giao th√¥ng ch·ªâ trong light zones
            red_detected_in_light_zone = False
            
            # Crop frame ch·ªâ trong light zones ƒë·ªÉ detect ƒë√®n
            for light_zone in light_zones.values():
                # T·∫°o mask cho light zone
                mask = np.zeros((h, w), dtype=np.uint8)
                cv2.fillPoly(mask, [light_zone["polygon"]], 255)
                
                # Crop frame theo light zone
                light_frame = cv2.bitwise_and(frame, frame, mask=mask)
                
                # Detect ƒë√®n trong v√πng n√†y
                light_results = model_light(light_frame)[0]
                zone_red_detected = any(
                    model_light.names[int(box.cls[0])].lower() == 'red' and float(box.conf[0]) > 0.5
                    for box in light_results.boxes
                )
                
                if zone_red_detected:
                    red_detected_in_light_zone = True
                    break
            
            red_light_history.append(red_detected_in_light_zone)
            if len(red_light_history) > 3:
                red_light_history.pop(0)
            is_red = red_light_history.count(True) > 1

            # Hi·ªÉn th·ªã tr·∫°ng th√°i ƒë√®n
            cv2.putText(frame_annotated, f"Red Light: {'YES' if is_red else 'NO'}",
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8,
                        (0, 0, 255) if is_red else (0, 255, 0), 2)

            # Hi·ªÉn th·ªã th√¥ng tin k√≠ch th∆∞·ªõc ƒë·ªÉ debug
            cv2.putText(frame_annotated, f"Frame: {w}x{h} | Standard: {STANDARD_WIDTH}x{STANDARD_HEIGHT}",
                        (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # Step 3: Detect v√† track ph∆∞∆°ng ti·ªán
            results = model_vehicle.track(source=frame, persist=True, conf=0.3, iou=0.5, tracker="bytetrack.yaml")[0]

            if not results.boxes or results.boxes.id is None:
                continue

            for i in range(len(results.boxes)):
                cls_id = int(results.boxes.cls[i])
                class_name = model_vehicle.names[cls_id]
                if class_name not in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                    continue

                x1, y1, x2, y2 = map(int, results.boxes.xyxy[i])
                cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
                track_id = int(results.boxes.id[i])

                # L∆∞u v·ªã tr√≠ tr∆∞·ªõc ƒë√≥
                prev_position = track_position_history.get(track_id)
                track_position_history[track_id] = (cx, cy)

                # X√°c ƒë·ªãnh zone hi·ªán t·∫°i c·ªßa vehicle
                current_zone_id = None
                for z_id, z in lane_zones.items():
                    if cv2.pointPolygonTest(z["polygon"], (cx, cy), False) >= 0:
                        current_zone_id = z_id
                        break

                # Ki·ªÉm tra chuy·ªÉn ƒë·ªông gi·ªØa c√°c zones
                prev_zone_id = track_zone_history.get(track_id)
                track_zone_history[track_id] = current_zone_id

                # Ki·ªÉm tra vi ph·∫°m v∆∞·ª£t ƒë√®n ƒë·ªè (v∆∞·ª£t qua line t·ª´ d∆∞·ªõi l√™n khi ƒë√®n ƒë·ªè)
                if is_red and prev_position and len(zone_lines) > 0:
                    for line_data in zone_lines:
                        line_coords = line_data["coordinates"]
                        if len(line_coords) >= 2:
                            line_start = tuple(line_coords[0])
                            line_end = tuple(line_coords[1])
                            
                            # Ki·ªÉm tra xe c√≥ v∆∞·ª£t qua line t·ª´ d∆∞·ªõi l√™n kh√¥ng
                            if detect_line_crossing(prev_position, (cx, cy), line_start, line_end):
                                # Ki·ªÉm tra xe c√≥ ƒëi ƒë√∫ng lane movement kh√¥ng
                                if (prev_zone_id and current_zone_id and 
                                    (prev_zone_id, current_zone_id) in lane_transitions):
                                    # Xe ƒëi ƒë√∫ng lane nh∆∞ng v∆∞·ª£t ƒë√®n ƒë·ªè
                                    vehicle_violations[track_id] = True
                                    vehicle_violation_types[track_id] = "RED_LIGHT"
                                    print(f"RED LIGHT VIOLATION: Vehicle {track_id} crossed line from zone {prev_zone_id} to {current_zone_id} during red light")

                # Ki·ªÉm tra vi ph·∫°m ƒëi sai l√†n (wrong way) khi ƒë√®n xanh
                if (not is_red and prev_zone_id and current_zone_id and 
                    prev_zone_id != current_zone_id and 
                    (prev_zone_id, current_zone_id) not in lane_transitions):
                    # Xe ƒëi sai lane khi ƒë√®n xanh
                    vehicle_violations[track_id] = True
                    vehicle_violation_types[track_id] = "WRONG_WAY"
                    print(f"WRONG WAY VIOLATION: Vehicle {track_id} moved from zone {prev_zone_id} to {current_zone_id} (not allowed movement)")

                # V·∫Ω bounding box v√† label
                violation_type = vehicle_violation_types.get(track_id, "")
                color = (0, 0, 255) if vehicle_violations.get(track_id) else (0, 255, 0)
                
                if vehicle_violations.get(track_id):
                    violation_text = violation_type
                else:
                    violation_text = "OK"
                    
                label = f"ID:{track_id} {class_name} {violation_text}"
                
                cv2.rectangle(frame_annotated, (x1, y1), (x2, y2), color, 2)
                cv2.putText(frame_annotated, label, (x1, y1 - 10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

                # Hi·ªÉn th·ªã zone hi·ªán t·∫°i
                if current_zone_id:
                    zone_text = f"Zone: {lane_zones[current_zone_id]['name']}"
                    cv2.putText(frame_annotated, zone_text, (x1, y2 + 20), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

            # Step 4: Stream video
            _, jpeg = cv2.imencode('.jpg', frame_annotated, [cv2.IMWRITE_JPEG_QUALITY, 80])
            yield (
                b"--frame\r\n"
                b"Content-Type: image/jpeg\r\n\r\n" + jpeg.tobytes() + b"\r\n"
            )

    finally:
        cap.release()